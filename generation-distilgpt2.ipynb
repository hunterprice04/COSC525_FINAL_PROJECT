{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Generation - DistilGPT2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "https://huggingface.co/docs/transformers/main/en/main_classes/text_generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W: Tokanizer wasn't specified explicitly, using same as model: distilgpt2\n",
      "# MEM BEFORE: (76.13% free): 8.00GB (total), 6.09GB (free), 1.91GB (used)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at distilgpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# MEM AFTER: (14.27% free): 8.00GB (total), 1.14GB (free), 6.86GB (used)\n"
     ]
    }
   ],
   "source": [
    "from utils.Utils import Utils\n",
    "from model.model import Model\n",
    "\n",
    "model = Model(model_name='distilgpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "# GENERATE:\n",
      "No initial prompt, generating random sequence.\n",
      "--------------------------------------------------------------------------------\n",
      "Outputs shape: (1, 20)\n",
      "================================================================================\n",
      "# OUTPUTS: len=1\n",
      "--------------------------------------------------------------------------------\n",
      "# OUTPUT[0]: len=89\n",
      "The U.S. Department of Justice has been investigating the death of a former FBI agent who\n"
     ]
    }
   ],
   "source": [
    "# do greedy decoding\n",
    "outputs = model.generate()\n",
    "Utils.print_outputs(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"What's up guys, in today's video\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "# GENERATE:\n",
      "--------------------------------------------------------------------------------\n",
      "Encoding prompt: len=32\n",
      "What's up guys, in today's video\n",
      "--------------------------------------------------------------------------------\n",
      "Outputs shape: (1, 100)\n",
      "================================================================================\n",
      "# OUTPUTS: len=1\n",
      "--------------------------------------------------------------------------------\n",
      "# OUTPUT[0]: len=383\n",
      "What's up guys, in today's video, and I'm going to talk about the new video, and I'm going to talk about the new video, and I'm going to talk about the new video, and I'm going to talk about the new video, and I'm going to talk about the new video, and I'm going to talk about the new video, and I'm going to talk about the new video, and I'm going to talk about the new video, and I\n"
     ]
    }
   ],
   "source": [
    "# generate 3 candidates using sampling\n",
    "outputs = model.generate(\n",
    "    prompt=prompt,\n",
    "    min_length=10,  # optional, defaults to 10\n",
    "    max_length=100,  # optional, defaults to model.config.max_length\n",
    ")\n",
    "\n",
    "Utils.print_outputs(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "# GENERATE:\n",
      "--------------------------------------------------------------------------------\n",
      "Encoding prompt: len=32\n",
      "What's up guys, in today's video\n",
      "--------------------------------------------------------------------------------\n",
      "Outputs shape: (1, 100)\n",
      "================================================================================\n",
      "# OUTPUTS: len=1\n",
      "--------------------------------------------------------------------------------\n",
      "# OUTPUT[0]: len=456\n",
      "What's up guys, in today's video, you're looking at the biggest video of each and every time the new generation of video-game consoles comes out. We're talking about the first generation of consoles that bring the industry along, but the thing that you're really looking at is the same thing.\"\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The Xbox 360 and Xbox One are just the few titles to finally break that mold. They've not only allowed people to play games but also let people play their own\n"
     ]
    }
   ],
   "source": [
    "# If do_sample=True, the generate method will use Sample Decoding.\n",
    "# Different decoding strategies here:\n",
    "# https://towardsdatascience.com/decoding-strategies-that-you-need-to-know-for-response-generation-ba95ee0faadc\n",
    "# If do_sample=False and num_beams=1, then the generate method will use greedy decoding.\n",
    "\n",
    "outputs = model.generate(\n",
    "    prompt=prompt,\n",
    "    max_length=100,\n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "Utils.print_outputs(outputs, strip=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "# OUTPUTS: len=1\n",
      "--------------------------------------------------------------------------------\n",
      "# OUTPUT[0]: len=452\n",
      "What's up guys, in today's video, you're looking at the biggest video of each and every time the new generation of video-game consoles comes out. We're talking about the first generation of consoles that bring the industry along, but the thing that you're really looking at is the same thing.\"\n",
      "The Xbox 360 and Xbox One are just the few titles to finally break that mold. They've not only allowed people to play games but also let people play their own\n"
     ]
    }
   ],
   "source": [
    "Utils.print_outputs(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "# GENERATE:\n",
      "--------------------------------------------------------------------------------\n",
      "Encoding prompt: len=32\n",
      "What's up guys, in today's video\n",
      "--------------------------------------------------------------------------------\n",
      "Outputs shape: (3, 100)\n",
      "================================================================================\n",
      "# OUTPUTS: len=3\n",
      "--------------------------------------------------------------------------------\n",
      "# OUTPUT[0]: len=273\n",
      "What's up guys, in today's video, it's up guys, in today's video, it's up guys, in today's video, it's up guys, in today's video, it's up guys, in today's video, it's up guys, in today's video, we're doing our job, and we're doing our job, and so we're going to do our job.\n",
      "--------------------------------------------------------------------------------\n",
      "# OUTPUT[1]: len=88\n",
      "What's up guys, in today's video, and I've got a lot of guys who want to play football.\"\n",
      "--------------------------------------------------------------------------------\n",
      "# OUTPUT[2]: len=106\n",
      "What's up guys, in today's video game industry you can see the latest trailer, a new game, and a new game.\n"
     ]
    }
   ],
   "source": [
    "# generate 3 candidates using sampling\n",
    "outputs = model.generate(\n",
    "    prompt=prompt,\n",
    "    max_length=100,\n",
    "    temperature=0.7,\n",
    "    num_return_sequences=3,\n",
    "    do_sample=True\n",
    ")\n",
    "Utils.print_outputs(outputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
