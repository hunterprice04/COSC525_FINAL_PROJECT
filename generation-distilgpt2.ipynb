{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Generation - DistilGPT2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "https://huggingface.co/docs/transformers/main/en/main_classes/text_generation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from utils.Utils import Utils\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W: Tokanizer wasn't specified explicitly, using same as model: distilgpt2\n",
      "# MEM BEFORE: (17.26% free): 8.00GB (total), 1.38GB (free), 6.62GB (used)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at distilgpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# MEM AFTER: (17.26% free): 8.00GB (total), 1.38GB (free), 6.62GB (used)\n"
     ]
    }
   ],
   "source": [
    "from model.model import Model\n",
    "\n",
    "model = Model(model_name='distilgpt2')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "# GENERATE:\n",
      "No initial prompt, generating random sequence.\n",
      "================================================================================\n",
      "# OUTPUTS:\n",
      "--------------------------------------------------------------------------------\n",
      "The U.S. Department of Justice has been investigating the death of a former FBI agent who\n"
     ]
    }
   ],
   "source": [
    "# do greedy decoding\n",
    "outputs = model.generate()\n",
    "Utils.print_outputs(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "prompt = \"What's up guys, in today's video\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "# GENERATE:\n",
      "--------------------------------------------------------------------------------\n",
      "Encoding prompt: len=32\n",
      "What's up guys, in today's video\n",
      "================================================================================\n",
      "# OUTPUTS: len=1\n",
      "--------------------------------------------------------------------------------\n",
      "# OUTPUT[0]: len=383\n",
      "What's up guys, in today's video, and I'm going to talk about the new video, and I'm going to talk about the new video, and I'm going to talk about the new video, and I'm going to talk about the new video, and I'm going to talk about the new video, and I'm going to talk about the new video, and I'm going to talk about the new video, and I'm going to talk about the new video, and I\n"
     ]
    }
   ],
   "source": [
    "# generate 3 candidates using sampling\n",
    "outputs = model.generate(\n",
    "    prompt=prompt,\n",
    "    min_length=10,  # optional, defaults to 10\n",
    "    max_length=100,  # optional, defaults to model.config.max_length\n",
    ")\n",
    "\n",
    "Utils.print_outputs(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "# GENERATE:\n",
      "--------------------------------------------------------------------------------\n",
      "Encoding prompt: len=32\n",
      "What's up guys, in today's video\n",
      "================================================================================\n",
      "# OUTPUTS: len=1\n",
      "--------------------------------------------------------------------------------\n",
      "# OUTPUT[0]: len=185\n",
      "What's up guys, in today's video, so that's how we talk about the whole process, so I can't wait to see where the hell this story goes.\"\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "H/T David\n",
      "\n",
      "\n",
      "\n",
      "Like this: Like Loading...\n"
     ]
    }
   ],
   "source": [
    "# If do_sample=True, the generate method will use Sample Decoding.\n",
    "# Different decoding strategies here:\n",
    "# https://towardsdatascience.com/decoding-strategies-that-you-need-to-know-for-response-generation-ba95ee0faadc\n",
    "# If do_sample=False and num_beams=1, then the generate method will use greedy decoding.\n",
    "\n",
    "outputs = model.generate(\n",
    "    prompt=prompt,\n",
    "    max_length=100,\n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "Utils.print_outputs(outputs, strip=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "# OUTPUTS: len=1\n",
      "--------------------------------------------------------------------------------\n",
      "# OUTPUT[0]: len=173\n",
      "What's up guys, in today's video, so that's how we talk about the whole process, so I can't wait to see where the hell this story goes.\"\n",
      "H/T David\n",
      "Like this: Like Loading...\n"
     ]
    }
   ],
   "source": [
    "Utils.print_outputs(outputs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "# GENERATE:\n",
      "--------------------------------------------------------------------------------\n",
      "Encoding prompt: len=32\n",
      "What's up guys, in today's video\n",
      "(3, 100)\n",
      "================================================================================\n",
      "# OUTPUTS: len=3\n",
      "--------------------------------------------------------------------------------\n",
      "# OUTPUT[0]: len=397\n",
      "What's up guys, in today's video, and I'm going to continue to talk about it. And then I will be back on the podcast, and I'll talk about some of the stuff that happens in the world, and what the world really needs to do to make sure things work out for everybody, and I'm going to talk about a lot of things that are happening to us.\n",
      "And it's kind of a conversation that I'm really excited about.\n",
      "--------------------------------------------------------------------------------\n",
      "# OUTPUT[1]: len=164\n",
      "What's up guys, in today's video I'll make sure to get your guys ready to fight another day.\n",
      "If you haven't seen the video please let us know in the comments below.\n",
      "--------------------------------------------------------------------------------\n",
      "# OUTPUT[2]: len=187\n",
      "What's up guys, in today's video you can see that there's a whole lot of fun. There's a lot of other stuff that we've done, but that's just about what we've done, so we're going to do it.\n"
     ]
    }
   ],
   "source": [
    "# generate 3 candidates using sampling\n",
    "outputs = model.generate(\n",
    "    prompt=prompt,\n",
    "    max_length=100,\n",
    "    temperature=0.7,\n",
    "    num_return_sequences=3,\n",
    "    do_sample=True\n",
    ")\n",
    "Utils.print_outputs(outputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}