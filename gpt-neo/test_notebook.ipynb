{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "GPTNeo_example_notebook.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "TPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J0i5MRP0SV8D",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Welcome to the colab notebook for [GPTNeo](https://github.com/EleutherAI/GPTNeo) - a fully open source implementation of GPT like models for mesh-tensorflow by [EleutherAI](eleuther.ai).\n",
    "\n",
    "Our library provides training and inference for GPT models up to GPT3 sizes on both TPUs and GPUs. \n",
    "\n",
    "In this notebook we walk you through TPU training (or finetuning!) and sampling using the freely available colab TPUs.\n",
    "\n",
    "If you find our repo useful, come join [our discord](https://discord.gg/BK2v3EJ) and say hi! ðŸ˜¬\n",
    "\n",
    "Before we get going - make sure you are running this notebook with a TPU available. Go to Runtime -> Change Runtime Type and select 'TPU' under hardware accelerator.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "K-53qkZV6Lv9",
    "cellView": "form",
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "source": [
    "#@title Setup\n",
    "!pip3 install -q -r requirements.txt\n",
    "pretrained_model = None\n",
    "dataset = None\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EZGbzUPD0tad",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Set Up Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R918l14UhrBR",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We first need to download and tokenize a dataset. If you just want to sample from a pretrained model, you can skip this step and move on to the `Pretrained Model` section.\n",
    "\n",
    "You can choose from:\n",
    "\n",
    "*   Sampling Only - choose this option if you only wish to sample from our trained models, then move on to the `Pretrained Model` section.\n",
    "\n",
    "*   OpenWebText - an opensource clone of OpenAI's WebText dataset, the original training data of GPT2.\n",
    "\n",
    "*   YoutubeSubtitles - a dataset of subtitles scraped from youtube videos.\n",
    "\n",
    "* Hackernews - comments scraped from hackernews\n",
    "\n",
    "* NIHExporter - Data relating to various projects from the national institute of health.\n",
    "\n",
    "* Custom - if this option is chosen you will be prompted to enter the path to your own dataset. It should be a directory containing .txt or .jsonl files.\n",
    "\n",
    "All these datasets are from EleutherAI's side project - [The Pileâ„¢](https://github.com/EleutherAI/The-Pile) - an effort to gather a general purpose, diverse and open source plain text dataset large enough to train 1T+ parameter language models.\n",
    "\n",
    "Even the smallest datasets are fairly large files, so this step will likely take a while. Select a dataset in the next cell, then run the next two cells, and go grab a snack and a cup of tea ðŸ˜Š\n",
    "\n",
    "Alternatively, you can provide your own dataset in the form of a folder or gzip archive of .txt files. Simply select 'Custom' below and follow input the path to your data and the name of your dataset when prompted."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "pM8jP3Am_hsx",
    "cellView": "form",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Select a Dataset:\n",
    "import os\n",
    "\n",
    "dataset = 'NIHExporter'  #@param [\"Sampling_Only\", \"OpenWebText\", \"YoutubeSubtitles\", \"HackerNews\", \"NIHExporter\", \"Custom\"]\n",
    "\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "if dataset == \"Sampling_Only\":\n",
    "    pass\n",
    "elif dataset == 'OpenWebText':\n",
    "    dpath = \"data/openwebtext.tar.xz\"\n",
    "    if not os.path.isfile(dpath):\n",
    "        !wget \"https://the-eye.eu/public/AI/pile_preliminary_components/openwebtext2.jsonl.zst.tar\" -O $dpath\n",
    "        !tar xf f\"{dpath}\"\n",
    "    else:\n",
    "        print(f\"Already exists: {dpath}\")\n",
    "    dataset_path = \"openwebtext\"\n",
    "    dataset_name = dataset_path\n",
    "    out_name = dataset_name + \"_tokenized\"\n",
    "elif dataset == 'YoutubeSubtitles':\n",
    "    dpath = \"data/yt_subs.jsonl.zst\"\n",
    "    if not os.path.isfile(dpath):\n",
    "        !wget \"https://the-eye.eu/public/AI/pile_preliminary_components/yt_subs.jsonl.zst\" -O $dpath\n",
    "    else:\n",
    "        print(f\"Already exists: {dpath}\")\n",
    "    dataset_path = 'data'\n",
    "    dataset_name = 'ytsubs'\n",
    "    out_name = dataset_name + \"_tokenized\"\n",
    "elif dataset == 'HackerNews':\n",
    "    dpath = \"data/hn.tar.gz\"\n",
    "    if not os.path.isfile(dpath):\n",
    "        !wget \"https://the-eye.eu/public/AI/pile_preliminary_components/hn.tar.gz\" -O $dpath\n",
    "    else:\n",
    "        print(f\"Already exists: {dpath}\")\n",
    "    dataset_path = 'data'\n",
    "    dataset_name = 'hackernews'\n",
    "    out_name = dataset_name + \"_tokenized\"\n",
    "elif dataset == \"NIHExporter\":\n",
    "    dpath = \"data/NIH_ExPORTER_awarded_grant_text.jsonl.zst\"\n",
    "    if not os.path.isfile(dpath):\n",
    "        !wget \"https://the-eye.eu/public/AI/pile_preliminary_components/NIH_ExPORTER_awarded_grant_text.jsonl.zst\" -O $dpath\n",
    "    else:\n",
    "        print(f\"Already exists: {dpath}\")\n",
    "    dataset_path = 'data'\n",
    "    dataset_name = 'nihexporter'\n",
    "    out_name = dataset_name + \"_tokenized\"\n",
    "elif dataset == \"Custom\":\n",
    "    dataset_path = input('Enter the path to the folder containing your data: ')\n",
    "    dataset_name = input('Enter the name of your dataset: ')\n",
    "    out_name = dataset_name + \"_tokenized\"\n",
    "else:\n",
    "    raise NotImplementedError(\n",
    "        'please select from available options: [\"OpenWebText\", \"YoutubeSubtitles\", \"HackerNews\", \"NIHExporter\", \"Custom\"]')\n"
   ],
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-04-06 12:49:43--  https://the-eye.eu/public/AI/pile_preliminary_components/NIH_ExPORTER_awarded_grant_text.jsonl.zst\r\n",
      "Resolving the-eye.eu (the-eye.eu)... 162.213.130.6\r\n",
      "Connecting to the-eye.eu (the-eye.eu)|162.213.130.6|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 630784092 (602M) [application/octet-stream]\r\n",
      "Saving to: â€˜data/NIH_ExPORTER_awarded_grant_text.jsonl.zstâ€™\r\n",
      "\r\n",
      "nt_text.jsonl.zst     5%[>                   ]  31.19M  4.07MB/s    eta 2m 42s ^C\r\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zMl1cHtN5I_W",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Tokenize Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6IBIompTJaqm",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now tokenize the dataset and copy it over to your google cloud bucket. You may skip this step if you are sampling from a pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Pq5u0WUSJWwz",
    "cellView": "both",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Tokenize Data\n",
    "!python \"data/create_tfrecords.py\" --input_dir \"/content/GPTNeo/$dataset_path\" --name $dataset_name --files_per 1000 --output_dir $out_name --write_dataset_config --processes 1"
   ],
   "execution_count": 7,
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'child' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/IPython/utils/_process_posix.py\u001B[0m in \u001B[0;36msystem\u001B[0;34m(self, cmd)\u001B[0m\n\u001B[1;32m    156\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 157\u001B[0;31m                 \u001B[0mchild\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpexpect\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mspawn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msh\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0margs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'-c'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcmd\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# Vanilla Pexpect\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    158\u001B[0m             \u001B[0mflush\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0msys\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstdout\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mflush\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/lib/python3/dist-packages/pexpect/pty_spawn.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, command, args, timeout, maxread, searchwindowsize, logfile, cwd, env, ignore_sighup, echo, preexec_fn, encoding, codec_errors, dimensions, use_poll)\u001B[0m\n\u001B[1;32m    203\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 204\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_spawn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpreexec_fn\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdimensions\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    205\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0muse_poll\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0muse_poll\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/lib/python3/dist-packages/pexpect/pty_spawn.py\u001B[0m in \u001B[0;36m_spawn\u001B[0;34m(self, command, args, preexec_fn, dimensions)\u001B[0m\n\u001B[1;32m    301\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 302\u001B[0;31m         self.ptyproc = self._spawnpty(self.args, env=self.env,\n\u001B[0m\u001B[1;32m    303\u001B[0m                                      cwd=self.cwd, **kwargs)\n",
      "\u001B[0;32m/usr/lib/python3/dist-packages/pexpect/pty_spawn.py\u001B[0m in \u001B[0;36m_spawnpty\u001B[0;34m(self, args, **kwargs)\u001B[0m\n\u001B[1;32m    313\u001B[0m         \u001B[0;34m'''Spawn a pty and return an instance of PtyProcess.'''\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 314\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mptyprocess\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPtyProcess\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mspawn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    315\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.8/dist-packages/ptyprocess/ptyprocess.py\u001B[0m in \u001B[0;36mspawn\u001B[0;34m(cls, argv, cwd, env, echo, preexec_fn, dimensions, pass_fds)\u001B[0m\n\u001B[1;32m    314\u001B[0m         \u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mclose\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mexec_err_pipe_write\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 315\u001B[0;31m         \u001B[0mexec_err_data\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mexec_err_pipe_read\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m4096\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    316\u001B[0m         \u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mclose\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mexec_err_pipe_read\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mUnboundLocalError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_25609/26589354.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m# Tokenize Data\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mget_ipython\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msystem\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'python data/create_tfrecords.py --input_dir /content/GPTNeo/$dataset_path --name $dataset_name --files_per 1000 --output_dir $out_name --write_dataset_config --processes 1'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;31m# copy the data to your bucket\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mpath_to_cloud_bucket\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mendswith\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'/'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/ipykernel/zmqshell.py\u001B[0m in \u001B[0;36msystem_piped\u001B[0;34m(self, cmd)\u001B[0m\n\u001B[1;32m    635\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0muser_ns\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'_exit_code'\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0msystem\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcmd\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    636\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 637\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0muser_ns\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'_exit_code'\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0msystem\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvar_expand\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcmd\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdepth\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    638\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    639\u001B[0m     \u001B[0;31m# Ensure new system_piped implementation is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/IPython/utils/_process_posix.py\u001B[0m in \u001B[0;36msystem\u001B[0;34m(self, cmd)\u001B[0m\n\u001B[1;32m    171\u001B[0m             \u001B[0;31m# (the character is known as ETX for 'End of Text', see\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    172\u001B[0m             \u001B[0;31m# curses.ascii.ETX).\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 173\u001B[0;31m             \u001B[0mchild\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msendline\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mchr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m3\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    174\u001B[0m             \u001B[0;31m# Read and print any more output the program might produce on its\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    175\u001B[0m             \u001B[0;31m# way out.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mUnboundLocalError\u001B[0m: local variable 'child' referenced before assignment"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NhvmTFD7b_fb",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Before starting training - you'll need to edit your dataset & model configs to point to your buckets / data. You need to do this even if you are sampling from a pre-trained model.\n",
    "\n",
    "*   First change the writefile path to point to your chosen dataset - e.g `%%writefile configs/dataset_configs/ytsubs.json`\n",
    "*   Change the \"path\" field to point to your cloud bucket location - e.g `gs://neo_lmdatasets/datasets/ytsubs_*.tfrecords`\n",
    "* Change `dataset_name` in `%%writefile configs/dataset_configs/dataset_name.json` to the name of your chosen dataset.\n",
    "* Once you've made the edits, then run the cell below to overwrite the existing files.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "MCsZP48vavCP",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "% % writefile \"configs/dataset_configs/Sampling_Only.json\"\n",
    "\n",
    "{\n",
    "    \"path\": \"gs://eleutherai/datasets/Sampling_Only/Sampling_Only*.tfrecords\",\n",
    "    \"eval_path\": \"\",\n",
    "    \"n_vocab\": 50256,\n",
    "    \"tokenizer_is_pretrained\": true,\n",
    "    \"tokenizer_path\": \"gpt2\",\n",
    "    \"eos_id\": 50256,\n",
    "    \"padding_id\": 50257\n",
    "}\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dH0x3dI9j85P",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Set Model Configs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I6GnCgAkB7GQ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The model below is identical to our pretrained GPT3XL model (1.3B Params). \n",
    "\n",
    "If you want to use a smaller model, you can modify any of the config files in ../configs/ ending in _8.json, all of which are designed to train on tpu-v8s.\n",
    "\n",
    "For a more detailed breakdown on what each item in the configuration file means - please read through our training and config guides in our [github README](https://github.com/EleutherAI/GPTNeo#training-guide). \n",
    "\n",
    "You'll want to change the first item in the `datasets` list to the name of your chosen dataset. (the filename minus .json in ./configs/dataset_configs)\n",
    "\n",
    "You'll also want to modify the `model_path` field to point to your google cloud bucket, so checkpoints get saved to there."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "L9hUDdokiWj6",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "% % writefile \"configs/GPT3_XL.json\"\n",
    "\n",
    "{\n",
    "    \"n_head\": 16,\n",
    "    \"n_vocab\": 50257,\n",
    "    \"embed_dropout\": 0,\n",
    "    \"lr\": 0.0002,\n",
    "    \"lr_decay\": \"cosine\",\n",
    "    \"warmup_steps\": 3000,\n",
    "    \"beta1\": 0.9,\n",
    "    \"beta2\": 0.95,\n",
    "    \"epsilon\": 1e-8,\n",
    "    \"opt_name\": \"adam\",\n",
    "    \"weight_decay\": 0,\n",
    "    \"train_batch_size\": 256,\n",
    "    \"attn_dropout\": 0,\n",
    "    \"train_steps\": 600000,\n",
    "    \"eval_steps\": 0,\n",
    "    \"predict_steps\": 1,\n",
    "    \"res_dropout\": 0,\n",
    "    \"eval_batch_size\": 4,\n",
    "    \"predict_batch_size\": 1,\n",
    "    \"iterations\": 100,\n",
    "    \"n_embd\": 2048,\n",
    "    \"datasets\": [[\"pile\", null, null, null]],\n",
    "    \"model\": \"GPT\",\n",
    "    \"model_path\": \"gs://eleutherai/GPT3_XL\",\n",
    "    \"n_ctx\": 2048,\n",
    "    \"n_layer\": 24,\n",
    "    \"scale_by_depth\": true,\n",
    "    \"scale_by_in\": false,\n",
    "    \"attention_types\": [[[\"global\", \"local\"], 12]],\n",
    "    \"mesh_shape\": \"x:4,y:2\",\n",
    "    \"layout\": \"intermediate_expanded:x,heads:x,vocab:n_vocab,memory_length:y,embd:y\",\n",
    "    \"activation_function\": \"gelu\",\n",
    "    \"recompute_grad\": true,\n",
    "    \"gradient_clipping\": 1.0,\n",
    "    \"tokens_per_mb_per_replica\": 2048,\n",
    "    \"precision\": \"bfloat16\"\n",
    "}"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GWK9MJqwcXKn",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Training from Scratch\n",
    "\n",
    "Now we will begin to train the model. If no previous model is found in \"model_path\", the model will start training from scratch. If you'd prefer to finetune from pretrained, skip to the `Finetune a Pretrained Model` section.\n",
    "\n",
    "If everything's set up correctly, you can now run the main.py function to start training!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "VUtrysOSBzjJ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "!python3 main.py --model colab_XL --steps_per_checkpoint 500 --tpu colab"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "koKQHA5ikCvD",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Pretrained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0QZv4_pnkk26",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "If you want to sample from or finetune a pretrained model, EleutherAI has pretrained two models for release. One with [1.3B parameters](https://the-eye.eu/public/AI/gptneo-release/GPT3_XL/), and another with [2.7B](https://the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/). \n",
    "\n",
    "Select an option below to download the weights locally. You will then need to upload them to your cloud bucket in order to finetune from them. If the download command isn't working, try the commented out code to download from a different source.\n",
    "\n",
    "The 2-7B model likely won't fit into the colab TPUs memory, and you may have to get some larger pods to finetune from it.\n",
    "\n",
    "Sampling from it, however, works just fine.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "lgTG1ammqGB0",
    "cellView": "form",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# @title Download pretrained model weights:\n",
    "pretrained_model = 'GPT3_2-7B'  #@param [\"GPT3_XL\", \"GPT3_2-7B\"]\n",
    "!wget -m -np -c -U \"eye02\" -w 2 -R \"index.html*\" \"https://the-eye.eu/public/AI/gptneo-release/$pretrained_model/\"\n",
    "path_to_local_weights = f\"/content/GPTNeo/the-eye.eu/public/AI/gptneo-release/{pretrained_model}\"\n",
    "\n",
    "# URL = f\"http://eaidata.bmk.sh/data/gptneo-release/{pretrained_model}/\"\n",
    "# FOLDER_NAME = \"GPT3_XL\"\n",
    "# !curl $URL | grep -i \"</a>\" | sed -n 's/.*href=\"\\([^\"]*\\).*/\\1/p' | sed \"s|^|$URL|\" | xargs -n 1 -P 4 wget -P $pretrained_model\n",
    "# path_to_local_weights = pretrained_model\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "GU3BDNJN_ZXE",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# upload to your bucket\n",
    "bucket_base = \"gs://\" + path_to_cloud_bucket.replace('gs://', '').split('/')[0]\n",
    "!gsutil -m cp -r $path_to_local_weights $bucket_base"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bnqkKBTOn0ox",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "If everything has worked successfully you should now see your model listed in your bucket below."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "80t9MMionm2h",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "!gsutil ls $bucket_base"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QDKL8fCSoApL",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now we want to make a few modifications to the model config in order to get training / sampling working on colab.\n",
    "\n",
    "If you are just sampling from our pretrained models, you can leave the settings as is, run the cell below, then move on to the `Sample from your model` section.\n",
    "\n",
    "If finetuning, you can change parameters below. \n",
    "\n",
    "* `path_to_model` should point to the model weights location in your cloud bucket, and will default to `$bucket_base/${pretrained_model}` if nothing is entered.\n",
    "\n",
    "* `batch_size` is your train batch size - if you're encountering memory errors, try lowering this.\n",
    "\n",
    "* `dataset_name` is the name of your dataset, if nothing is entered, this should default to the dataset you selected in the `Prepare Data` section.\n",
    "\n",
    "* `mesh_shape` specifies the way the model will be divided up across the TPU cores. We suggest leaving this alone unless you know what you're doing.\n",
    "\n",
    "* `train_steps` specifies how many steps you want the model to finetune for. We set this to 1000 for demonstrative purposes but you may need to increase this a little depending on your goals. If you are just sampling from the model, you can leave this as is.\n",
    "\n",
    "* `steps_per_checkpoint` specifies how often you want to save model weights during training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Laf0slBMDCUj",
    "cellView": "form",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# @title Modify config for colab. \n",
    "\n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "path_to_model = \"\"  #@param {type:\"string\"}\n",
    "batch_size = 8  #@param {type:\"integer\"}\n",
    "dset = \"\"  #@param {type:\"string\"}\n",
    "mesh_shape = \"x:4,y:2\"  #@param {type:\"string\"}\n",
    "train_steps = 1000  #@param {type:\"integer\"}\n",
    "steps_per_checkpoint = 500  #@param {type:\"integer\"}\n",
    "start_step = 400000 if pretrained_model == \"GPT3_2-7B\" else 362000\n",
    "\n",
    "if path_to_model == \"\":\n",
    "    path_to_model = f'{bucket_base.strip(\"/\")}/{pretrained_model}'\n",
    "print(f'MODEL PATH: {path_to_model}\\n')\n",
    "\n",
    "if dset == \"\" and dataset != \"Sampling_Only\":\n",
    "    dset = dataset\n",
    "elif dataset is None and dset == \"\":\n",
    "    dset = \"pile\"\n",
    "\n",
    "\n",
    "def pad_to_multiple_of(n, mult):\n",
    "    \"\"\"\n",
    "    pads n to a multiple of mult\n",
    "    \"\"\"\n",
    "    extra = n % mult\n",
    "    if extra > 0:\n",
    "        n = n + mult - extra\n",
    "    return n\n",
    "\n",
    "\n",
    "with open(f'{path_to_local_weights}/config.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "    pprint(data)\n",
    "    dset_val = [[dset, None, None, None]] if dset != \"\" else data[\"datasets\"]\n",
    "    mods = {\n",
    "        \"mesh_shape\": mesh_shape,\n",
    "        \"layout\": \"intermediate_expanded:x,heads:x,memory_length:y,embd:y\",\n",
    "        \"model_path\": path_to_model,\n",
    "        \"datasets\": dset_val,\n",
    "        \"train_steps\": start_step + train_steps,\n",
    "        \"eval_steps\": 0,\n",
    "        \"train_batch_size\": batch_size,\n",
    "        \"predict_batch_size\": batch_size\n",
    "    }\n",
    "    data.update(mods)\n",
    "    print('\\n--->\\n')\n",
    "    pprint(data)\n",
    "    with open(f'configs/{pretrained_model}.json', 'w') as outfile:\n",
    "        json.dump(data, outfile, indent=2)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fPwwbPCA6O7r",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Begin Fine-Tuning\n",
    "\n",
    "If you are fine-tuning the pretrained model, this line of code will begin the training."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0YlaHzyXuMaj",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "!python3 main.py --model $pretrained_model --steps_per_checkpoint $steps_per_checkpoint --tpu colab"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I_HxtEmBGTGT",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Sample from your model\n",
    "\n",
    "Once training is finished, (or your pretrained model is on your bucket), you can run the same command with the --predict flag to sample from your model.\n",
    "\n",
    "To pass in a prompt, save it to a .txt file, and pass in the name of the file with the --prompt flag.\n",
    "\n",
    "use the cell below to enter your prompt, and run it to save it to example_prompt.txt.\n",
    "\n",
    "You may need to decrease the predict batch size in your config if you're facing OOM errors.\n",
    "\n",
    "Let's see if the GPTNeo model can finish coding itself, with a sample prompt consisting of the beginning of a `torch.nn.Module`:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "CQE1Y5wPFx7h",
    "outputId": "e1a92c0c-18ee-4014-a0b8-d67161384940",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "% % writefile example_prompt.txt\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    \"\"\"  the full GPT language model, with a context size of block_size \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        # input embedding stem\n",
    "        self.tok_emb = nn.Embedding(config.vocab_size, config.n_embd)\n",
    "        self.pos_emb = nn.Parameter(torch.zeros(1, config.block_size, config.n_embd))\n",
    "        self.drop = nn.Dropout(config.embd_pdrop)\n",
    "        # transformer\n",
    "        self.blocks = nn.Sequential(*[Block(config) for _ in range(config.n_layer)])\n",
    "        # decoder head\n",
    "        self.ln_f = nn.LayerNorm(config.n_embd)\n",
    "        self.head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "        self.block_size = config.block_size\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "        logger.info(\"number of parameters: %e\", sum(p.numel() for p in self.parameters()))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "sf_5E4fHFQIh",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "f3c12a94-7ef8-43c1-a668-6365966d42b4",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "!python3 main.py --model $pretrained_model --steps_per_checkpoint 500 --tpu colab --predict --prompt example_prompt.txt"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nE9VImzHaI0z",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Evaluating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XGGbkgaFfp6f",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This section assumes you are using a pretrained model and relies on variables created in the `Pretrained model` section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I45yUIpbaLUJ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Wikitext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zwBDB9U2keFV",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Download the wikitext test set:\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "uuugiBmJaNxf",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "wikitext103_src = \"https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-raw-v1.zip\"\n",
    "!wget $wikitext103_src\n",
    "!unzip wikitext-103-raw-v1.zip"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J5wf3QWKkhZt",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Tokenize and upload to bucket:\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6mo8UUtDdctH",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "\n",
    "!mkdir wikitext\n",
    "!mv \"/content/GPTNeo/wikitext-103-raw/wiki.test.raw\" \"wikitext/wikitext_test.txt\"\n",
    "\n",
    "# Tokenize Data\n",
    "!python \"data/create_tfrecords.py\" --input_dir wikitext --name wikitext --files_per 1000 --output_dir wikitext_tokenized --write_dataset_config --processes 1 --wikitext-detokenize\n",
    "\n",
    "# copy the data to your bucket\n",
    "if not path_to_cloud_bucket.endswith('/'):\n",
    "    path_to_cloud_bucket += '/'\n",
    "copy_loc = path_to_cloud_bucket\n",
    "!gsutil -m cp -r wikitext_tokenized $copy_loc\n",
    "!gsutil ls $path_to_cloud_bucket"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GE84TUd1fAzf",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now make a dataset config that points to the tokenized wikitext data:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Z5UU7DQeeY0S",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "% % writefile \"configs/dataset_configs/wikitext.json\"\n",
    "\n",
    "{\n",
    "    \"path\": \"\",\n",
    "    \"eval_path\": \"gs://test-bucket-neo/wikitext_tokenized/*.tfrecords\",\n",
    "    \"n_vocab\": 50256,\n",
    "    \"tokenizer_is_pretrained\": true,\n",
    "    \"tokenizer_path\": \"gpt2\",\n",
    "    \"eos_id\": 50256,\n",
    "    \"padding_id\": 50257\n",
    "}\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "egvdwIOqfFER",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "And update your model config to point to that dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cellView": "form",
    "id": "AtdoIFMgfOe8",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# @title Modify config for wikitext. \n",
    "\n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "batch_size = 8  #@param {type:\"integer\"}\n",
    "assert pretrained_model is not None\n",
    "with open(f'configs/{pretrained_model}.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "    pprint(data)\n",
    "    dset_val = [[\"wikitext\", None, None, None]]\n",
    "    mods = {\n",
    "        \"datasets\": dset_val,\n",
    "        \"eval_steps\": 139 // batch_size,\n",
    "        \"train_batch_size\": batch_size,\n",
    "        \"eval_batch_size\": batch_size,\n",
    "    }\n",
    "    data.update(mods)\n",
    "    print('\\n--->\\n')\n",
    "    pprint(data)\n",
    "    with open(f'configs/{pretrained_model}.json', 'w') as outfile:\n",
    "        json.dump(data, outfile, indent=2)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U2d5eTHEg6Xj",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now run model in eval mode over tokenized data:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "s1Uz3PXzg5Pm",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "!python3 main.py --eval --tpu colab --model $pretrained_model"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9dbkPVcMhVaR",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Lambada\n",
    "\n",
    "Lambada eval is built into the codebase and can be run by adding a field to your model config"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cellView": "form",
    "id": "z4FJXOlJiEYo",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# @title Modify config for Lambada. \n",
    "\n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "batch_size = 8  #@param {type:\"integer\"}\n",
    "assert pretrained_model is not None\n",
    "with open(f'configs/{pretrained_model}.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "    mods = {\n",
    "        \"datasets\": dset_val,\n",
    "        \"eval_steps\": 0,\n",
    "        \"train_batch_size\": batch_size,\n",
    "        \"eval_batch_size\": batch_size,\n",
    "        \"eval_tasks\": [\"lambada\"]\n",
    "    }\n",
    "    data.update(mods)\n",
    "    print('\\n--->\\n')\n",
    "    pprint(data)\n",
    "    with open(f'configs/{pretrained_model}.json', 'w') as outfile:\n",
    "        json.dump(data, outfile, indent=2)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Upp-bGMriVPK",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now run the eval:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "OOA1YZDRiUhN",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "!python3 main.py --eval --tpu colab --model $pretrained_model"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}