{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformers installation\n",
    "! pip install transformers datasets\n",
    "# To install from source instead of the last release, comment the command above and uncomment the following one.\n",
    "# ! pip install git+https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick tour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¬°Entra en marcha con los ü§ó Transformers! Comienza usando `pipeline()`para una inferencia veloz, carga un modelo preentrenado y un tokenizador con una [AutoClass](https://huggingface.co/docs/transformers/main/es/./model_doc/auto) para resolver tu tarea de texto, visi√≥n o audio.\n",
    "\n",
    "<Tip>\n",
    "\n",
    "Todos los ejemplos de c√≥digo presentados en la documentaci√≥n tienen un bot√≥n arriba a la izquierda para elegir entre Pytorch y TensorFlow.\n",
    "Si no fuese as√≠, se espera que el c√≥digo funcione para ambos backends sin ning√∫n cambio.\n",
    "\n",
    "</Tip>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pipeline()`es la forma m√°s f√°cil de usar un modelo preentrenado para una tarea dada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/tiZFewofSLM?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@title\n",
    "from IPython.display import HTML\n",
    "\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/tiZFewofSLM?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El `pipeline()`soporta muchas tareas comunes listas para usar:\n",
    "\n",
    "**Texto**:\n",
    "* An√°lisis de Sentimientos: clasifica la polaridad de un texto dado.\n",
    "* Generaci√≥n de texto (solo en ingl√©s): genera texto a partir de un input dado.\n",
    "* Name entity recognition (NER): etiqueta cada palabra con la entidad que representa (persona, fecha, ubicaci√≥n, etc.).\n",
    "* Responder preguntas: extrae la respuesta del contexto dado un contexto y una pregunta.\n",
    "* Fill-mask: rellena el espacio faltante dado un texto con palabras enmascaradas.\n",
    "* Summarization: genera un resumen de una secuencia larga de texto o un documento.\n",
    "* Traducci√≥n: traduce un texto a otro idioma.\n",
    "* Extracci√≥n de caracter√≠sticas: crea una representaci√≥n tensorial del texto.\n",
    "\n",
    "**Imagen**:\n",
    "* Clasificaci√≥n de im√°genes: clasifica una imagen.\n",
    "* Segmentaci√≥n de im√°genes: clasifica cada pixel de una imagen.\n",
    "* Detecci√≥n de objetos: detecta objetos dentro de una imagen.\n",
    "\n",
    "**Audio**:\n",
    "* Clasificaci√≥n de audios: asigna una etiqueta a un segmento de audio.\n",
    "* Automatic speech recognition (ASR): transcribe datos de audio a un texto.\n",
    "\n",
    "<Tip>\n",
    "\n",
    "Para m√°s detalles acerca del `pipeline()`y tareas asociadas, consulta la documentaci√≥n [aqu√≠](https://huggingface.co/docs/transformers/main/es/./main_classes/pipelines).\n",
    "\n",
    "</Tip>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uso del Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el siguiente ejemplo, usar√°s el `pipeline()`para an√°lisis de sentimiento.\n",
    "\n",
    "Instala las siguientes dependencias si a√∫n no lo has hecho:\n",
    "\n",
    "```bash\n",
    "pip install torch\n",
    "```\n",
    "```bash\n",
    "pip install tensorflow\n",
    "```\n",
    "\n",
    "Importa `pipeline()`y especifica la tarea que deseas completar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El pipeline descarga y almacena en cach√© un [modelo preentrenado](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english) por defecto y tokeniza para an√°lisis de sentimiento. Ahora puedes usar `classifier` en tu texto objetivo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(\"We are very happy to show you the ü§ó Transformers library.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para m√°s de un enunciado entrega una lista de frases al `pipeline()`que devolver√° una lista de diccionarios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label: POSITIVE, with score: 0.9998\n",
       "label: NEGATIVE, with score: 0.5309"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = classifier([\"We are very happy to show you the ü§ó Transformers library.\", \"We hope you don't hate it.\"])\n",
    "for result in results:\n",
    "    print(f\"label: {result['label']}, with score: {round(result['score'], 4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El `pipeline()`tambi√©n puede iterar sobre un dataset entero. Comienza instalando la biblioteca [ü§ó Datasets](https://huggingface.co/docs/datasets/):\n",
    "\n",
    "```bash\n",
    "pip install datasets\n",
    "```\n",
    "\n",
    "Crea un `pipeline()`con la tarea que deseas resolver y el modelo que quieres usar. Coloca el par√°metro `device` a `0` para poner los tensores en un dispositivo CUDA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "speech_recognizer = pipeline(\"automatic-speech-recognition\", model=\"facebook/wav2vec2-base-960h\", device=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuaci√≥n, carga el dataset (ve ü§ó Datasets [Quick Start](https://huggingface.co/docs/datasets/quickstart.html) para m√°s detalles) sobre el que quisieras iterar. Por ejemplo, vamos a cargar el dataset [SUPERB](https://huggingface.co/datasets/superb):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "dataset = datasets.load_dataset(\"superb\", name=\"asr\", split=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puedes pasar un pipeline para un dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'HE HOPED THERE WOULD BE STEW FOR DINNER TURNIPS AND CARROTS AND BRUISED POTATOES AND FAT MUTTON PIECES TO BE LADLED OUT IN THICK PEPPERED FLOWER FAT AND SAUCE'},\n",
       " {'text': 'STUFFERED INTO YOU HIS BELLY COUNSELLED HIM'},\n",
       " {'text': 'AFTER EARLY NIGHTFALL THE YELLOW LAMPS WOULD LIGHT UP HERE AND THERE THE SQUALID QUARTER OF THE BROTHELS'},\n",
       " {'text': 'HO BERTIE ANY GOOD IN YOUR MIND'}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = dataset[\"file\"]\n",
    "speech_recognizer(files[:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para un dataset m√°s grande, donde los inputs son de mayor tama√±o (como en habla/audio o visi√≥n), querr√°s pasar un generador en lugar de una lista que carga todos los inputs en memoria. Ve la [documentaci√≥n del pipeline](https://huggingface.co/docs/transformers/main/es/./main_classes/pipelines) para m√°s informaci√≥n."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use otro modelo y otro tokenizador en el pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El `pipeline()`puede adaptarse a cualquier modelo del [Model Hub](https://huggingface.co/models) haciendo m√°s f√°cil adaptar el `pipeline()`para otros casos de uso. Por ejemplo, si quisieras un modelo capaz de manejar texto en franc√©s, usa los tags en el Model Hub para filtrar entre los modelos apropiados. El resultado mejor filtrado devuelve un [modelo BERT](https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment) multilingual fine-tuned para el an√°lisis de sentimiento. Genial, ¬°vamos a usar este modelo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usa `AutoModelForSequenceClassification`y ['AutoTokenizer'] para cargar un modelo preentrenado y un tokenizador asociado (m√°s en un `AutoClass` debajo):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usa `TFAutoModelForSequenceClassification`y ['AutoTokenizer'] para cargar un modelo preentrenado y un tokenizador asociado (m√°s en un `TFAutoClass` debajo):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despu√©s puedes especificar el modelo y el tokenizador en el `pipeline()` y aplicar el `classifier` en tu texto objetivo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': '5 stars', 'score': 0.7273}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "classifier(\"Nous sommes tr√®s heureux de vous pr√©senter la biblioth√®que ü§ó Transformers.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si no pudieras encontrar el modelo para tu caso respectivo de uso necesitar√°s ajustar un modelo preentrenado a tus datos. Mira nuestro [tutorial de fine-tuning](https://huggingface.co/docs/transformers/main/es/./training) para aprender c√≥mo. Finalmente, despu√©s de que has ajustado tu modelo preentrenado, ¬°por favor considera compartirlo (ve el tutorial [aqu√≠](https://huggingface.co/docs/transformers/main/es/./model_sharing)) con la comunidad en el Model Hub para democratizar el NLP! ü§ó"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoClass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/AhChOFRegn4?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@title\n",
    "from IPython.display import HTML\n",
    "\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/AhChOFRegn4?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debajo del cap√≥, las clases `AutoModelForSequenceClassification`y `AutoTokenizer`trabajan juntas para dar poder al `pipeline()` Una [AutoClass](https://huggingface.co/docs/transformers/main/es/./model_doc/auto) es un atajo que autom√°ticamente recupera la arquitectura de un modelo preentrenado con su nombre o el path. S√≥lo necesitar√°s seleccionar el `AutoClass` apropiado para tu tarea y tu tokenizador asociado con `AutoTokenizer`\n",
    "\n",
    "Regresemos a nuestro ejemplo y veamos c√≥mo puedes usar el `AutoClass` para reproducir los resultados del `pipeline()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un tokenizador es responsable de procesar el texto a un formato que sea entendible para el modelo. Primero, el tokenizador separar√° el texto en palabras llamadas *tokens*. Hay m√∫ltiples reglas que gobiernan el proceso de tokenizaci√≥n incluyendo el c√≥mo separar una palabra y en qu√© nivel (aprende m√°s sobre tokenizaci√≥n [aqu√≠](https://huggingface.co/docs/transformers/main/es/./tokenizer_summary)). Lo m√°s importante es recordar que necesitar√°s instanciar el tokenizador con el mismo nombre del modelo para asegurar que est√°s usando las mismas reglas de tokenizaci√≥n con las que el modelo fue preentrenado.\n",
    "\n",
    "Carga un tokenizador con `AutoTokenizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despu√©s, el tokenizador convierte los tokens a n√∫meros para construir un tensor que servir√° como input para el modelo. Esto es conocido como el *vocabulario* del modelo.\n",
    "\n",
    "Pasa tu texto al tokenizador:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 11312, 10320, 12495, 19308, 10114, 11391, 10855, 10103, 100, 58263, 13299, 119, 102],\n",
       " 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding = tokenizer(\"We are very happy to show you the ü§ó Transformers library.\")\n",
    "print(encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El tokenizador devolver√° un diccionario conteniendo:\n",
    "\n",
    "* [input_ids](https://huggingface.co/docs/transformers/main/es/./glossary#input-ids): representaciones num√©ricas de los tokens.\n",
    "* [atttention_mask](https://huggingface.co/docs/transformers/main/es/.glossary#attention-mask): indica cu√°les tokens deben ser atendidos.\n",
    "\n",
    "Como con el `pipeline()` el tokenizador aceptar√° una lista de inputs. Adem√°s, el tokenizador tambi√©n puede rellenar (pad, en ingl√©s) y truncar el texto para devolver un lote (batch, en ingl√©s) de longitud uniforme:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_batch = tokenizer(\n",
    "    [\"We are very happy to show you the ü§ó Transformers library.\", \"We hope you don't hate it.\"],\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    "    return_tensors=\"pt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_batch = tokenizer(\n",
    "    [\"We are very happy to show you the ü§ó Transformers library.\", \"We hope you don't hate it.\"],\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    "    return_tensors=\"tf\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lee el tutorial de [preprocessing](https://huggingface.co/docs/transformers/main/es/./preprocessing) para m√°s detalles acerca de la tokenizaci√≥n."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AutoModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ü§ó Transformers provee una forma simple y unificada de cargar tus instancias preentrenadas. Esto significa que puedes cargar un `AutoModel`como cargar√≠as un `AutoTokenizer` La √∫nica diferencia es seleccionar el `AutoModel`correcto para la tarea. Ya que est√°s clasificando texto, o secuencias, carga `AutoModelForSequenceClassification`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "pt_model = AutoModelForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Tip>\n",
    "\n",
    "Ve el [task summary](https://huggingface.co/docs/transformers/main/es/./task_summary) para revisar qu√© clase del `AutoModel`deber√≠as usar para cada tarea.\n",
    "\n",
    "</Tip>\n",
    "\n",
    "Ahora puedes pasar tu lote (batch) preprocesado de inputs directamente al modelo. Solo tienes que desempacar el diccionario a√±adiendo `**`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_outputs = pt_model(**pt_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo producir√° las activaciones finales en el atributo `logits`. Aplica la funci√≥n softmax a `logits` para obtener las probabilidades:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0021, 0.0018, 0.0115, 0.2121, 0.7725],\n",
       "        [0.2084, 0.1826, 0.1969, 0.1755, 0.2365]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "pt_predictions = nn.functional.softmax(pt_outputs.logits, dim=-1)\n",
    "print(pt_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ü§ó Transformers provee una forma simple y unificada de cargar tus instancias preentrenadas. Esto significa que puedes cargar un `TFAutoModel`como cargar√≠as un `AutoTokenizer` La √∫nica diferencia es seleccionar el `TFAutoModel`correcto para la tarea. Ya que est√°s clasificando texto, o secuencias, carga `TFAutoModelForSequenceClassification`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFAutoModelForSequenceClassification\n",
    "\n",
    "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "tf_model = TFAutoModelForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Tip>\n",
    "  Ve el [task summary](https://huggingface.co/docs/transformers/main/es/./task_summary) para revisar qu√© clase del `AutoModel`  deber√≠as usar para cada tarea.\n",
    "</Tip>\n",
    "\n",
    "Ahora puedes pasar tu lote preprocesado de inputs directamente al modelo pasando las llaves del diccionario directamente a los tensores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_outputs = tf_model(tf_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo producir√° las activaciones finales en el atributo `logits`. Aplica la funci√≥n softmax a `logits` para obtener las probabilidades:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tf.Tensor(\n",
       "[[0.0021 0.0018 0.0116 0.2121 0.7725]\n",
       " [0.2084 0.1826 0.1969 0.1755  0.2365]], shape=(2, 5), dtype=float32)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf_predictions = tf.nn.softmax(tf_outputs.logits, axis=-1)\n",
    "print(tf.math.round(tf_predictions * 10**4) / 10**4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Tip>\n",
    "\n",
    "Todos los modelos de ü§ó Transformers (PyTorch o TensorFlow) producir√°n los tensores *antes* de la funci√≥n de activaci√≥n\n",
    "final (como softmax) porque la funci√≥n de activaci√≥n final es com√∫nmente fusionada con la p√©rdida.\n",
    "\n",
    "</Tip>\n",
    "\n",
    "Los modelos son [`torch.nn.Module`](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) o [`tf.keras.Model`](https://www.tensorflow.org/api_docs/python/tf/keras/Model) est√°ndares as√≠ que podr√°s usarlos en tu training loop usual. Sin embargo, para facilitar las cosas, ü§ó Transformers provee una clase `Trainer`para PyTorch que a√±ade funcionalidades para entrenamiento distribuido, precici√≥n mixta, y m√°s. Para TensorFlow, puedes usar el m√©todo `fit` desde [Keras](https://keras.io/). Consulta el [tutorial de entrenamiento](https://huggingface.co/docs/transformers/main/es/./training) para m√°s detalles.\n",
    "\n",
    "<Tip>\n",
    "\n",
    "Los outputs del modelo de ü§ó Transformers son dataclasses especiales por lo que sus atributos pueden ser completados en un IDE.\n",
    "Los outputs del modelo tambi√©n se comportan como tuplas o diccionarios (e.g., puedes indexar con un entero, un slice o una cadena) en cuyo caso los atributos que son `None` son ignorados.\n",
    "\n",
    "</Tip>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guarda un modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez que tu modelo est√© fine-tuned puedes guardarlo con tu tokenizador usando `PreTrainedModel.save_pretrained()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_save_directory = \"./pt_save_pretrained\"\n",
    "tokenizer.save_pretrained(pt_save_directory)\n",
    "pt_model.save_pretrained(pt_save_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando quieras usar el modelo otra vez c√°rgalo con `PreTrainedModel.from_pretrained()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_model = AutoModelForSequenceClassification.from_pretrained(\"./pt_save_pretrained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez que tu modelo est√© fine-tuned puedes guardarlo con tu tokenizador usando `TFPreTrainedModel.save_pretrained()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_save_directory = \"./tf_save_pretrained\"\n",
    "tokenizer.save_pretrained(tf_save_directory)\n",
    "tf_model.save_pretrained(tf_save_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando quieras usar el modelo otra vez c√°rgalo con `TFPreTrainedModel.from_pretrained()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_model = TFAutoModelForSequenceClassification.from_pretrained(\"./tf_save_pretrained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una caracter√≠stica particularmente cool de ü§ó Transformers es la habilidad de guardar el modelo y cargarlo como un modelo de PyTorch o TensorFlow. El par√°metro `from_pt` o `from_tf` puede convertir el modelo de un framework al otro:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(tf_save_directory)\n",
    "pt_model = AutoModelForSequenceClassification.from_pretrained(tf_save_directory, from_tf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFAutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(pt_save_directory)\n",
    "tf_model = TFAutoModelForSequenceClassification.from_pretrained(pt_save_directory, from_pt=True)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
