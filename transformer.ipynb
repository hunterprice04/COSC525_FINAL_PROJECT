{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# NLG Transformer\n",
    "[https://www.tensorflow.org/text/tutorials/transformer](https://www.tensorflow.org/text/tutorials/transformer)\n",
    "[\"Attention is All You Need\"](https://arxiv.org/abs/1706.03762)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@formatter:off\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "#@formatter:on\n",
    "\n",
    "import tensorflow as tf\n",
    "from src.utils.Utils import Utils\n",
    "\n",
    "Utils.tensorflow_shutup()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## [Multi-Head Attention](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MultiHeadAttention)\n",
    "\n",
    "```\n",
    "tf.keras.layers.MultiHeadAttention(\n",
    "    num_heads,\n",
    "    key_dim,\n",
    "    value_dim=None,\n",
    "    dropout=0.0,\n",
    "    use_bias=True,\n",
    "    output_shape=None,\n",
    "    attention_axes=None,\n",
    "    kernel_initializer='glorot_uniform',\n",
    "    bias_initializer='zeros',\n",
    "    kernel_regularizer=None,\n",
    "    bias_regularizer=None,\n",
    "    activity_regularizer=None,\n",
    "    kernel_constraint=None,\n",
    "    bias_constraint=None,\n",
    "    **kwargs\n",
    ")\n",
    "```"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<KerasTensor: shape=(None, 5, 3, 4, 16) dtype=float32 (created by layer 'multi_head_attention_3')>, <KerasTensor: shape=(None, 5, 2, 3, 4, 3, 4) dtype=float32 (created by layer 'multi_head_attention_3')>)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/qq/60_vklcs1d591jyh7j2s0sp80000gn/T/ipykernel_55992/2240315573.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[0moutput_tensor\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlayer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minput_tensor\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput_tensor\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreturn_attention_scores\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     16\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moutput_tensor\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 17\u001B[0;31m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moutput_tensor\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     18\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     19\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'tuple' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "\n",
    "# query shape: (batch_size, <query dimensions>, key_dim),\n",
    "query = tf.random.uniform((32, 10, 10))  # (B, T, dim)\n",
    "# key shape: (batch_size, <key/value dimensions>, key_dim),\n",
    "key = tf.random.uniform((32, 10, 10))  # (B, S, dim)\n",
    "# value shape: (batch_size, <key/value dimensions>, value_dim)\n",
    "value = tf.random.uniform((32, 10, 10))  # (B, S, dim)\n",
    "\n",
    "# If query, key, value are the same, then this is self-attention\n",
    "\n",
    "# Each timestep in query attends to the corresponding sequence in key, and returns a fixed-width vector.\n",
    "layer = tf.keras.layers.MultiHeadAttention(num_heads=2, key_dim=2, attention_axes=(2, 3))\n",
    "input_tensor = tf.keras.Input(shape=[5, 3, 4, 16])\n",
    "output_tensor = layer(input_tensor, input_tensor, return_attention_scores=True)\n",
    "print(output_tensor)\n",
    "print(output_tensor.shape)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "got_ver is None",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/qq/60_vklcs1d591jyh7j2s0sp80000gn/T/ipykernel_55992/2781806901.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0mtransformers\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mGPT2TokenizerFast\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0mt_gpt2_fast\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mGPT2TokenizerFast\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfrom_pretrained\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"EleutherAI/gpt-neo-1.3B\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.9/site-packages/transformers/__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     28\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     29\u001B[0m \u001B[0;31m# Check the dependencies satisfy the minimal versions required.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 30\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0;34m.\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mdependency_versions_check\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     31\u001B[0m from .utils import (\n\u001B[1;32m     32\u001B[0m     \u001B[0m_LazyModule\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.9/site-packages/transformers/dependency_versions_check.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     39\u001B[0m                 \u001B[0;32mcontinue\u001B[0m  \u001B[0;31m# not required, check version only if installed\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     40\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 41\u001B[0;31m         \u001B[0mrequire_version_core\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdeps\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mpkg\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     42\u001B[0m     \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     43\u001B[0m         \u001B[0;32mraise\u001B[0m \u001B[0mValueError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf\"can't find {pkg} in {deps.keys()}, check dependency_versions_table.py\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.9/site-packages/transformers/utils/versions.py\u001B[0m in \u001B[0;36mrequire_version_core\u001B[0;34m(requirement)\u001B[0m\n\u001B[1;32m    118\u001B[0m     \u001B[0;34m\"\"\"require_version wrapper which emits a core-specific hint on failure\"\"\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    119\u001B[0m     \u001B[0mhint\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m\"Try: pip install transformers -U or pip install -e '.[dev]' if you're working with git main\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 120\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0mrequire_version\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrequirement\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mhint\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m/usr/local/lib/python3.9/site-packages/transformers/utils/versions.py\u001B[0m in \u001B[0;36mrequire_version\u001B[0;34m(requirement, hint)\u001B[0m\n\u001B[1;32m    112\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mwant_ver\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    113\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mop\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mwant_ver\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mwanted\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mitems\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 114\u001B[0;31m             \u001B[0m_compare_versions\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mop\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgot_ver\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mwant_ver\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrequirement\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpkg\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mhint\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    115\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    116\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.9/site-packages/transformers/utils/versions.py\u001B[0m in \u001B[0;36m_compare_versions\u001B[0;34m(op, got_ver, want_ver, requirement, pkg, hint)\u001B[0m\n\u001B[1;32m     43\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0m_compare_versions\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mop\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgot_ver\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mwant_ver\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrequirement\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpkg\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mhint\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     44\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mgot_ver\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 45\u001B[0;31m         \u001B[0;32mraise\u001B[0m \u001B[0mValueError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"got_ver is None\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     46\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mwant_ver\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     47\u001B[0m         \u001B[0;32mraise\u001B[0m \u001B[0mValueError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"want_ver is None\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mValueError\u001B[0m: got_ver is None"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "t_gpt2_fast = GPT2TokenizerFast.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}