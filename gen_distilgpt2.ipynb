{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Generation - DistilGPT2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "https://huggingface.co/docs/transformers/main/en/main_classes/text_generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W: Tokanizer wasn't specified explicitly, using same as model: distilgpt2\n",
      "GPU-0: 7007.06/8192.0 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at distilgpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU-0: 7007.06/8192.0 MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from src.models.HFModel import HFModel\n",
    "\n",
    "model = HFModel(model_name='distilgpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "# GENERATE:\n",
      "No initial prompt, generating random sequence.\n",
      "--------------------------------------------------------------------------------\n",
      "Outputs shape: (1, 20)\n",
      "================================================================================\n",
      "# OUTPUTS: len=1\n",
      "--------------------------------------------------------------------------------\n",
      "# OUTPUT[0]: len=89\n",
      "The U.S. Department of Justice has been investigating the death of a former FBI agent who\n"
     ]
    }
   ],
   "source": [
    "from src.utils.ModelUtils import ModelUtils\n",
    "\n",
    "# do greedy decoding\n",
    "outputs = model.generate()\n",
    "ModelUtils.print_outputs(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"What's up guys, in today's video\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "# GENERATE:\n",
      "--------------------------------------------------------------------------------\n",
      "Encoding prompt: len=32\n",
      "What's up guys, in today's video\n",
      "--------------------------------------------------------------------------------\n",
      "Outputs shape: (1, 100)\n",
      "================================================================================\n",
      "# OUTPUTS: len=1\n",
      "--------------------------------------------------------------------------------\n",
      "# OUTPUT[0]: len=383\n",
      "What's up guys, in today's video, and I'm going to talk about the new video, and I'm going to talk about the new video, and I'm going to talk about the new video, and I'm going to talk about the new video, and I'm going to talk about the new video, and I'm going to talk about the new video, and I'm going to talk about the new video, and I'm going to talk about the new video, and I\n"
     ]
    }
   ],
   "source": [
    "# generate 3 candidates using sampling\n",
    "outputs = model.generate(\n",
    "    prompt=prompt,\n",
    "    min_length=10,  # optional, defaults to 10\n",
    "    max_length=100,  # optional, defaults to model.config.max_length\n",
    ")\n",
    "\n",
    "ModelUtils.print_outputs(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "# GENERATE:\n",
      "--------------------------------------------------------------------------------\n",
      "Encoding prompt: len=32\n",
      "What's up guys, in today's video\n",
      "--------------------------------------------------------------------------------\n",
      "Outputs shape: (1, 100)\n",
      "================================================================================\n",
      "# OUTPUTS: len=1\n",
      "--------------------------------------------------------------------------------\n",
      "# OUTPUT[0]: len=193\n",
      "What's up guys, in today's video of their meeting with the group's director of music, Markiele Williams. We're just really hoping they get something this year. [Photo courtesy of the band, MTV]\n"
     ]
    }
   ],
   "source": [
    "# If do_sample=True, the generate method will use Sample Decoding.\n",
    "# Different decoding strategies here:\n",
    "# https://towardsdatascience.com/decoding-strategies-that-you-need-to-know-for-response-generation-ba95ee0faadc\n",
    "# If do_sample=False and num_beams=1, then the generate method will use greedy decoding.\n",
    "\n",
    "outputs = model.generate(\n",
    "    prompt=prompt,\n",
    "    max_length=100,\n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "ModelUtils.print_outputs(outputs, strip=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "# OUTPUTS: len=1\n",
      "--------------------------------------------------------------------------------\n",
      "# OUTPUT[0]: len=193\n",
      "What's up guys, in today's video of their meeting with the group's director of music, Markiele Williams. We're just really hoping they get something this year. [Photo courtesy of the band, MTV]\n"
     ]
    }
   ],
   "source": [
    "ModelUtils.print_outputs(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "# GENERATE:\n",
      "--------------------------------------------------------------------------------\n",
      "Encoding prompt: len=32\n",
      "What's up guys, in today's video\n",
      "--------------------------------------------------------------------------------\n",
      "Outputs shape: (3, 100)\n",
      "================================================================================\n",
      "# OUTPUTS: len=3\n",
      "--------------------------------------------------------------------------------\n",
      "# OUTPUT[0]: len=296\n",
      "What's up guys, in today's video, that's a big thing. And I don't want to get into what's going on here, other than the fight, and it's been a different job for me. I just want to get into it, and I want to say, I'm going to do whatever's going on here, and I want to do whatever's going on here.\n",
      "--------------------------------------------------------------------------------\n",
      "# OUTPUT[1]: len=413\n",
      "What's up guys, in today's video, that would make it hard to get to the point where you can't even get to the point where you can't even get to the point where you can't even get to the point where you can't even get to the point where you can't even get to the point where you can't even get to the point where you can't even get to the point where you can't even get to the point where you can't even get to the\n",
      "--------------------------------------------------------------------------------\n",
      "# OUTPUT[2]: len=331\n",
      "What's up guys, in today's video, you can hear me getting pretty excited about all the awesome things that are happening in this video. This is the first of many videos I've done, and this is one of the first videos I've been able to share with you.\n",
      "What's your favorite part of this video?\n",
      "Thanks,\n",
      "John-\n",
      "I'm so excited to be back!\n"
     ]
    }
   ],
   "source": [
    "# generate 3 candidates using sampling\n",
    "outputs = model.generate(\n",
    "    prompt=prompt,\n",
    "    max_length=100,\n",
    "    temperature=0.7,\n",
    "    num_return_sequences=3,\n",
    "    do_sample=True\n",
    ")\n",
    "ModelUtils.print_outputs(outputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}