{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Generation - DistilGPT2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "https://huggingface.co/docs/transformers/main/en/main_classes/text_generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W: Tokanizer wasn't specified explicitly, using same as model: distilgpt2\n",
      "GPU-0: 7003.06/8192.0 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at distilgpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU-0: 7003.06/8192.0 MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from src.models.HFModel import HFModel\n",
    "\n",
    "model = HFModel(model_name='distilgpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "# GENERATE:\n",
      "No initial prompt, generating random sequence.\n",
      "--------------------------------------------------------------------------------\n",
      "Outputs shape: (1, 20)\n",
      "================================================================================\n",
      "# OUTPUTS: len=1\n",
      "--------------------------------------------------------------------------------\n",
      "# OUTPUT[0]: len=89\n",
      "The U.S. Department of Justice has been investigating the death of a former FBI agent who\n"
     ]
    }
   ],
   "source": [
    "from src.utils.ModelUtils import ModelUtils\n",
    "\n",
    "# do greedy decoding\n",
    "outputs = model.generate()\n",
    "ModelUtils.print_outputs(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"What's up guys, in today's video\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "# GENERATE:\n",
      "--------------------------------------------------------------------------------\n",
      "Encoding prompt: len=32\n",
      "What's up guys, in today's video\n",
      "--------------------------------------------------------------------------------\n",
      "Outputs shape: (1, 100)\n",
      "================================================================================\n",
      "# OUTPUTS: len=1\n",
      "--------------------------------------------------------------------------------\n",
      "# OUTPUT[0]: len=383\n",
      "What's up guys, in today's video, and I'm going to talk about the new video, and I'm going to talk about the new video, and I'm going to talk about the new video, and I'm going to talk about the new video, and I'm going to talk about the new video, and I'm going to talk about the new video, and I'm going to talk about the new video, and I'm going to talk about the new video, and I\n"
     ]
    }
   ],
   "source": [
    "# generate 3 candidates using sampling\n",
    "outputs = model.generate(\n",
    "    prompt=prompt,\n",
    "    min_length=10,  # optional, defaults to 10\n",
    "    max_length=100,  # optional, defaults to model.config.max_length\n",
    ")\n",
    "\n",
    "ModelUtils.print_outputs(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "# GENERATE:\n",
      "--------------------------------------------------------------------------------\n",
      "Encoding prompt: len=32\n",
      "What's up guys, in today's video\n",
      "--------------------------------------------------------------------------------\n",
      "Outputs shape: (1, 18)\n",
      "================================================================================\n",
      "# OUTPUTS: len=1\n",
      "--------------------------------------------------------------------------------\n",
      "# OUTPUT[0]: len=56\n",
      "What's up guys, in today's video, in the game industry?‍\n"
     ]
    }
   ],
   "source": [
    "# If do_sample=True, the generate method will use Sample Decoding.\n",
    "# Different decoding strategies here:\n",
    "# https://towardsdatascience.com/decoding-strategies-that-you-need-to-know-for-response-generation-ba95ee0faadc\n",
    "# If do_sample=False and num_beams=1, then the generate method will use greedy decoding.\n",
    "\n",
    "outputs = model.generate(\n",
    "    prompt=prompt,\n",
    "    max_length=100,\n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "ModelUtils.print_outputs(outputs, strip=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "# OUTPUTS: len=1\n",
      "--------------------------------------------------------------------------------\n",
      "# OUTPUT[0]: len=56\n",
      "What's up guys, in today's video, in the game industry?‍\n"
     ]
    }
   ],
   "source": [
    "ModelUtils.print_outputs(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "# GENERATE:\n",
      "--------------------------------------------------------------------------------\n",
      "Encoding prompt: len=32\n",
      "What's up guys, in today's video\n",
      "--------------------------------------------------------------------------------\n",
      "Outputs shape: (3, 100)\n",
      "================================================================================\n",
      "# OUTPUTS: len=3\n",
      "--------------------------------------------------------------------------------\n",
      "# OUTPUT[0]: len=132\n",
      "What's up guys, in today's video, in tonight's video, and in the night we're talking about a couple of events, the UFC, and the UFC.\n",
      "--------------------------------------------------------------------------------\n",
      "# OUTPUT[1]: len=213\n",
      "What's up guys, in today's video, what happened to the kids and the guys in the hall,\" he says. \"They didn't think we'd be able to do that. I'm not certain we'd be able to do that. We just didn't get the job done.\n",
      "--------------------------------------------------------------------------------\n",
      "# OUTPUT[2]: len=349\n",
      "What's up guys, in today's video, where I'm making a video about people and how it will help me get to the top of my game.\n",
      "(Photo courtesy of TheWrap)\n",
      "That's the one game I want to make.\n",
      "The goal is to make up for it by making the game as easy as possible.\n",
      "I'm not going to do whatever I want to do, but I'll make it easy by making it easy and easy.\n"
     ]
    }
   ],
   "source": [
    "# generate 3 candidates using sampling\n",
    "outputs = model.generate(\n",
    "    prompt=prompt,\n",
    "    max_length=100,\n",
    "    temperature=0.7,\n",
    "    num_return_sequences=3,\n",
    "    do_sample=True\n",
    ")\n",
    "ModelUtils.print_outputs(outputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}