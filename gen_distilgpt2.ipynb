{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Generation - DistilGPT2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "https://huggingface.co/docs/transformers/main/en/main_classes/text_generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W: Tokanizer wasn't specified explicitly, using same as model: distilgpt2\n",
      "# MEM BEFORE: (14.47% free): 8.00GB (total), 1.16GB (free), 6.84GB (used)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at distilgpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# MEM AFTER: (14.47% free): 8.00GB (total), 1.16GB (free), 6.84GB (used)\n"
     ]
    }
   ],
   "source": [
    "from utils.Utils import Utils\n",
    "from model.model import HFModel\n",
    "\n",
    "model = HFModel(model_name='distilgpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "# GENERATE:\n",
      "No initial prompt, generating random sequence.\n",
      "--------------------------------------------------------------------------------\n",
      "Outputs shape: (1, 20)\n",
      "================================================================================\n",
      "# OUTPUTS: len=1\n",
      "--------------------------------------------------------------------------------\n",
      "# OUTPUT[0]: len=89\n",
      "The U.S. Department of Justice has been investigating the death of a former FBI agent who\n"
     ]
    }
   ],
   "source": [
    "# do greedy decoding\n",
    "outputs = model.generate()\n",
    "Utils.print_outputs(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"What's up guys, in today's video\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "# GENERATE:\n",
      "--------------------------------------------------------------------------------\n",
      "Encoding prompt: len=32\n",
      "What's up guys, in today's video\n",
      "--------------------------------------------------------------------------------\n",
      "Outputs shape: (1, 100)\n",
      "================================================================================\n",
      "# OUTPUTS: len=1\n",
      "--------------------------------------------------------------------------------\n",
      "# OUTPUT[0]: len=383\n",
      "What's up guys, in today's video, and I'm going to talk about the new video, and I'm going to talk about the new video, and I'm going to talk about the new video, and I'm going to talk about the new video, and I'm going to talk about the new video, and I'm going to talk about the new video, and I'm going to talk about the new video, and I'm going to talk about the new video, and I\n"
     ]
    }
   ],
   "source": [
    "# generate 3 candidates using sampling\n",
    "outputs = model.generate(\n",
    "    prompt=prompt,\n",
    "    min_length=10,  # optional, defaults to 10\n",
    "    max_length=100,  # optional, defaults to model.config.max_length\n",
    ")\n",
    "\n",
    "Utils.print_outputs(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "# GENERATE:\n",
      "--------------------------------------------------------------------------------\n",
      "Encoding prompt: len=32\n",
      "What's up guys, in today's video\n",
      "--------------------------------------------------------------------------------\n",
      "Outputs shape: (1, 100)\n",
      "================================================================================\n",
      "# OUTPUTS: len=1\n",
      "--------------------------------------------------------------------------------\n",
      "# OUTPUT[0]: len=431\n",
      "What's up guys, in today's video they talk about the next step as they move into 2018, the future of the MLS. This is just an opinion I take in a moment that really takes on a lot of people, and that's what's going on with the league in today's video on the big stage as we just watched this video. So, if you'd like to do one thing about this scene, just keep making your decision before the minute, I guess when I actually listen\n"
     ]
    }
   ],
   "source": [
    "# If do_sample=True, the generate method will use Sample Decoding.\n",
    "# Different decoding strategies here:\n",
    "# https://towardsdatascience.com/decoding-strategies-that-you-need-to-know-for-response-generation-ba95ee0faadc\n",
    "# If do_sample=False and num_beams=1, then the generate method will use greedy decoding.\n",
    "\n",
    "outputs = model.generate(\n",
    "    prompt=prompt,\n",
    "    max_length=100,\n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "Utils.print_outputs(outputs, strip=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "# OUTPUTS: len=1\n",
      "--------------------------------------------------------------------------------\n",
      "# OUTPUT[0]: len=431\n",
      "What's up guys, in today's video they talk about the next step as they move into 2018, the future of the MLS. This is just an opinion I take in a moment that really takes on a lot of people, and that's what's going on with the league in today's video on the big stage as we just watched this video. So, if you'd like to do one thing about this scene, just keep making your decision before the minute, I guess when I actually listen\n"
     ]
    }
   ],
   "source": [
    "Utils.print_outputs(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "# GENERATE:\n",
      "--------------------------------------------------------------------------------\n",
      "Encoding prompt: len=32\n",
      "What's up guys, in today's video\n",
      "--------------------------------------------------------------------------------\n",
      "Outputs shape: (3, 100)\n",
      "================================================================================\n",
      "# OUTPUTS: len=3\n",
      "--------------------------------------------------------------------------------\n",
      "# OUTPUT[0]: len=74\n",
      "What's up guys, in today's video?\"\n",
      "Watch a special edition of the podcast.\n",
      "--------------------------------------------------------------------------------\n",
      "# OUTPUT[1]: len=386\n",
      "What's up guys, in today's video interview with the Real Estate and The Dealers of The Dealers of The Dealers of The Dealers of The Dealers of The Dealers of The Dealers of The Dealers of The Dealers of The Dealers of The Dealers of The Dealers of The Dealers of The Dealers of the Dealers of The Dealers of The Dealers of The Dealers of The Dealers of The Dealers of The Dealers of The\n",
      "--------------------------------------------------------------------------------\n",
      "# OUTPUT[2]: len=391\n",
      "What's up guys, in today's video, the first thing the guys need is a good man. And that's why I'm here, so I'm here with the guys who are playing a decent role for the club to show us how much they care about the organization.\"\n",
      "\"The club has gone into this business because it's a really good organization, and I really don't like to believe in that kind of thing,\" he said.\n",
      "\"I don't want to\n"
     ]
    }
   ],
   "source": [
    "# generate 3 candidates using sampling\n",
    "outputs = model.generate(\n",
    "    prompt=prompt,\n",
    "    max_length=100,\n",
    "    temperature=0.7,\n",
    "    num_return_sequences=3,\n",
    "    do_sample=True\n",
    ")\n",
    "Utils.print_outputs(outputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
