{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Playing with RNN"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from src import TextDataset, TextGeneration, RNN, LSTM, Sampling, Config\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"1\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Reading TextDataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataConfig(WINDOW_SIZE=35, STRIDE=3, DATA_PATH='data/beatles.txt')\n",
      "TrainingConfig(HIDDEN_STATE_SIZE=[256, 512], EPOCHS=250, BATCH_SIZE=2500, DROPOUT=0.5, LR=0.001, BUFFER_SIZE=10000, SAVE_DIR='models', PRED_EVERY=5, PRED_LEN=100, PRED_TEMP=0.75)\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    'DATA': {\n",
    "        'WINDOW_SIZE': 35,\n",
    "        'STRIDE': 3,\n",
    "        'DATA_PATH': os.path.join(\"data\", \"beatles.txt\"),\n",
    "    },\n",
    "    \"TRAINING\": {\n",
    "        \"HIDDEN_STATE_SIZE\": [256, 512],\n",
    "        \"BATCH_SIZE\": 2500,\n",
    "        \"EPOCHS\": 250,\n",
    "    },\n",
    "}\n",
    "config = Config(**config)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# [TextDataset]:\tConfig(DATA=DataConfig(WINDOW_SIZE=35, STRIDE=3, DATA_PATH='data/beatles.txt'), TRAINING=TrainingConfig(HIDDEN_STATE_SIZE=[256, 512], EPOCHS=250, BATCH_SIZE=2500, DROPOUT=0.5, LR=0.001, BUFFER_SIZE=10000, SAVE_DIR='models', PRED_EVERY=5, PRED_LEN=100, PRED_TEMP=0.75), EMBED=None)\n",
      "\t166753 total chars in text\n",
      "# [Embeddings - init]:\tlabel=beatles\tencoding=utf-8\n",
      "# Dateset Summary [label=`beatles`]:\n",
      "=> Configuration:\n",
      "\tDataConfig(WINDOW_SIZE=35, STRIDE=3, DATA_PATH='data/beatles.txt')\n",
      "=> 10 Samples:\n",
      "\t1. b'a day in the life\\ni read the news t' => b' day in the life\\ni read the news to'\n",
      "\t2. b'ay in the life\\ni read the news toda' => b'y in the life\\ni read the news today'\n",
      "\t3. b'in the life\\ni read the news today o' => b'n the life\\ni read the news today oh'\n",
      "\t4. b'the life\\ni read the news today oh b' => b'he life\\ni read the news today oh bo'\n",
      "\t5. b' life\\ni read the news today oh boy\\n' => b'life\\ni read the news today oh boy\\na'\n",
      "\t6. b'fe\\ni read the news today oh boy\\nabo' => b'e\\ni read the news today oh boy\\nabou'\n",
      "\t7. b'i read the news today oh boy\\nabout ' => b' read the news today oh boy\\nabout a'\n",
      "\t8. b'ead the news today oh boy\\nabout a l' => b'ad the news today oh boy\\nabout a lu'\n",
      "\t9. b' the news today oh boy\\nabout a luck' => b'the news today oh boy\\nabout a lucky'\n",
      "\t10. b'e news today oh boy\\nabout a lucky m' => b' news today oh boy\\nabout a lucky ma'\n",
      "# Vocab Summary [label=`beatles`]:\n",
      "\t* 49 unique chars\n",
      "\t* 26 alpha\t10 numeric\t12 symbols\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = TextDataset(config, verbosity=1).read()\n",
    "print(data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Peek at the One-Hot Encoded Data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MapDataset shapes: ((35, 49), (35, 49)), types: (tf.int64, tf.int64)>\n",
      "(<tf.Tensor: shape=(35, 49), dtype=int64, numpy=\n",
      "array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 1, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 1, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]])>, <tf.Tensor: shape=(35, 49), dtype=int64, numpy=\n",
      "array([[0, 0, 1, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 1, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]])>)\n",
      "(<tf.Tensor: shape=(35, 49), dtype=int64, numpy=\n",
      "array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 1, 0, 0],\n",
      "       [0, 0, 1, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]])>, <tf.Tensor: shape=(35, 49), dtype=int64, numpy=\n",
      "array([[0, 0, 0, ..., 1, 0, 0],\n",
      "       [0, 0, 1, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 1, 0, 0]])>)\n",
      "(<tf.Tensor: shape=(35, 49), dtype=int64, numpy=\n",
      "array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 1, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 1, 0, 0],\n",
      "       [0, 0, 1, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]])>, <tf.Tensor: shape=(35, 49), dtype=int64, numpy=\n",
      "array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 1, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 1, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]])>)\n",
      "(<tf.Tensor: shape=(35, 49), dtype=int64, numpy=\n",
      "array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 1, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]])>, <tf.Tensor: shape=(35, 49), dtype=int64, numpy=\n",
      "array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 1, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 1, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]])>)\n",
      "(<tf.Tensor: shape=(35, 49), dtype=int64, numpy=\n",
      "array([[0, 0, 1, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 1, 0, 0],\n",
      "       [0, 1, 0, ..., 0, 0, 0]])>, <tf.Tensor: shape=(35, 49), dtype=int64, numpy=\n",
      "array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 1, 0, 0],\n",
      "       [0, 1, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]])>)\n"
     ]
    }
   ],
   "source": [
    "print(data.dataset_oh)\n",
    "for i in data.dataset_oh.take(5):\n",
    "    print(i)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Generating Text - Dry Run"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# [TextGeneration]:\tmodel_func=get_LSTM\tverbosity=1\n"
     ]
    }
   ],
   "source": [
    "generator = TextGeneration(config=config, model_func=LSTM.get_LSTM, verbosity=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "generator.train(data, config.TRAINING)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No models found in models/beatles",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_3821281/1490001560.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m# Load model from models/beatles/model_LSTM_beatles_WS-35_ST-3_BS-2500_HS-256-512_DR-0.5_LR-0.001\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mgenerator\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload_model\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"models\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"beatles\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m/jupyter/COSC525_TEAM/project4/src/TextGeneration.py\u001B[0m in \u001B[0;36mload_model\u001B[0;34m(self, path, index)\u001B[0m\n\u001B[1;32m    181\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    182\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mmodels\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 183\u001B[0;31m             \u001B[0;32mraise\u001B[0m \u001B[0mValueError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf\"No models found in {path}\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    184\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    185\u001B[0m         \u001B[0moptions\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodels\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mkeys\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mValueError\u001B[0m: No models found in models/beatles"
     ]
    }
   ],
   "source": [
    "# Load model from models/beatles/model_LSTM_beatles_WS-35_ST-3_BS-2500_HS-256-512_DR-0.5_LR-0.001\n",
    "generator.load_model(os.path.join(\"models\", \"beatles\"), 0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "# [TextGeneration - predict]:\tprompt_len=24\tnum_chars=500\n",
      "Old Function\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating...: 100%|██████████| 500/500 [00:17<00:00, 28.50it/s, char=[b'u']]           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "i swear, man, this ain't got nothing to say but it’s o.k.\n",
      "good morning, good morning...\n",
      "people running round it's five o’clock.\n",
      "everywhere in town with you\n",
      "i don’t know why you say goodbye, i say hello\n",
      "hello hello\n",
      "i don't know why you say goodbye, i say hello\n",
      "hello hello\n",
      "i don't know why you say goodbye, i say hello\n",
      "hello hello\n",
      "i don't know why you say goodbye, i say hello\n",
      "hello hello\n",
      "i don't know why you say goodbye, i say hello\n",
      "hello hello\n",
      "i don't know why you say goodbye, i say hello\n",
      "hello hello\n",
      "i don't know why you\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.set_random_seed(0)\n",
    "prompt = \"i swear, man, this ain't\"\n",
    "print(\"=\" * 80)\n",
    "generated = generator.predict(prompt, sampling_method=Sampling.greedy_search, pred_len=500,\n",
    "                              include_prompt=True)\n",
    "print(\"=\" * 80)\n",
    "print(generated)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "# [TextGeneration - predict]:\tprompt_len=24\tnum_chars=500\n",
      "Old Function\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating...: 100%|██████████| 500/500 [00:17<00:00, 28.48it/s, char=[b'e']]           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "i swear, man, this ain't no dancer.\n",
      "look out helter skelter\n",
      "helter skelter\n",
      "look out helter skelter\n",
      "helter skelter\n",
      "let me have to say that i’ve been unhappy with you\n",
      "i see no day\n",
      "if i till your feet.\n",
      "roll up to make a doe who is all belong?\n",
      "sit beside a mountain waiting for this moment to arise\n",
      "you were only waiting for this moment to arise.\n",
      "blackbird singing in the dead of night\n",
      "take these broken wings and learn to see\n",
      "all the cold and to you.\n",
      "all my loving, darling\n",
      "if you wanna dance with me\n",
      "if you wanna dance with me\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.set_random_seed(0)\n",
    "prompt = \"i swear, man, this ain't\"\n",
    "print(\"=\" * 80)\n",
    "generated = generator.predict(prompt, sampling_method=Sampling.random_sampling_bugged, pred_len=500,\n",
    "                              include_prompt=True)\n",
    "print(\"=\" * 80)\n",
    "print(generated)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "# [TextGeneration - predict]:\tprompt_len=24\tnum_chars=500\n",
      "Old Function\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating...: 100%|██████████| 500/500 [00:18<00:00, 27.68it/s, char=[b' ']]           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "i swear, man, this ain't the kind that you can see.\n",
      "whoa, oh, these chains of love got a hold on me.\n",
      "believe me when i tell you\n",
      "i’m looking through you, you’re never weep at night i said these words to my girl,\n",
      "i have got another man\n",
      "that’s the end’a little girl\n",
      "help you should see polythene pam\n",
      "you better run for your life if you can, little girl\n",
      "and why you lied to me,\n",
      "tell me why\n",
      "you can send it here\n",
      "but i never heard it singing\n",
      "no i never heard it at all.\n",
      "would it out of my window\n",
      "you’re coming home, you’re coming \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.set_random_seed(0)\n",
    "prompt = \"i swear, man, this ain't\"\n",
    "print(\"=\" * 80)\n",
    "generated = generator.predict(prompt, sampling_method=Sampling.random_sampling, pred_len=500,\n",
    "                              temp=0.75,\n",
    "                              include_prompt=True)\n",
    "print(\"=\" * 80)\n",
    "print(generated)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}