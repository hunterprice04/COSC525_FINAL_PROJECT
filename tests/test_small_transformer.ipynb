{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Testing Custom Transformer\n",
    "https://huggingface.co/docs/datasets/quickstart"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#@formatter:off\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "#@formatter:on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/huggingface/bin/python\r\n"
     ]
    }
   ],
   "source": [
    "# this need to point to your env with hugging face package installed\n",
    "!which python"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import sys\n",
    "import pathlib\n",
    "import os\n",
    "sys.path.append(os.path.join(pathlib.Path('.').parent.resolve(),'..'))\n",
    "from src import Sampling\n",
    "from src import SamplingEnums as ENUMS\n",
    "\n",
    "import tensorflow as tf\n",
    "from transformers import GPT2Tokenizer, SmallTransformerConfig\n",
    "from transformers.models.small_transformer.modeling_tf_small_transformer import (\n",
    "    TF_SMALL_TRANSFORMER_PRETRAINED_MODEL_ARCHIVE_LIST,\n",
    "    TFSmallTransformerLMHeadModel,\n",
    ")\n",
    "from transformers.tf_utils import shape_list\n",
    "from datasets import load_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset wikitext (/Users/hp/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n",
      "Reusing dataset wikitext (/Users/hp/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n"
     ]
    }
   ],
   "source": [
    "train_dataset_raw = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
    "test_dataset_raw = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "data": {
      "text/plain": "Dataset({\n    features: ['text'],\n    num_rows: 36718\n})"
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_raw"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tokenize the dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/hp/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-eb43a12640473c0e.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/5 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2184347b315c49f5b50e53b5f29f3681"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "BATCH_SIZE = 32\n",
    "dataset = train_dataset_raw.map(lambda e: tokenizer(e['text'], truncation=True, padding='max_length'), batched=True)\n",
    "edataset = test_dataset_raw.map(lambda e: tokenizer(e['text'], truncation=True, padding='max_length'), batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")\n",
    "train_dataset = dataset.to_tf_dataset(\n",
    "    columns=['input_ids', 'attention_mask'],\n",
    "    shuffle=True,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "test_dataset = dataset.to_tf_dataset(\n",
    "    columns=['input_ids', 'attention_mask'],\n",
    "    shuffle=True,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    collate_fn=data_collator,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[50256 50256 50256 ... 50256 50256 50256]\n",
      " [  796   796 45205 ... 50256 50256 50256]\n",
      " [50256 50256 50256 ... 50256 50256 50256]\n",
      " ...\n",
      " [  383 24933   319 ... 50256 50256 50256]\n",
      " [50256 50256 50256 ... 50256 50256 50256]\n",
      " [ 9415   358 23490 ... 50256 50256 50256]]\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [1 1 1 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [1 1 1 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [1 1 1 ... 0 0 0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-16 00:00:07.608248: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    }
   ],
   "source": [
    "for i in train_dataset.take(1).as_numpy_iterator():\n",
    "    print(i['input_ids'])\n",
    "    print(i['attention_mask'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Format the Dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [],
   "source": [
    "# CONFIGURATION VARIABLES ( CHANGE THESE - THESE ARE USED IN THE MODEL TESTING FILE)\n",
    "batch_size = 13\n",
    "seq_length = 7\n",
    "is_training = True\n",
    "use_token_type_ids = True\n",
    "use_input_mask = True\n",
    "use_labels = True\n",
    "use_mc_token_ids = True\n",
    "vocab_size = 99\n",
    "hidden_size = 32\n",
    "num_hidden_layers = 5\n",
    "num_attention_heads = 4\n",
    "intermediate_size = 37\n",
    "hidden_act = \"gelu\"\n",
    "hidden_dropout_prob = 0.1\n",
    "attention_probs_dropout_prob = 0.1\n",
    "max_position_embeddings = 512\n",
    "type_vocab_size = 16\n",
    "type_sequence_label_size = 2\n",
    "initializer_range = 0.02\n",
    "num_labels = 3\n",
    "num_choices = 4\n",
    "scope = None\n",
    "bos_token_id = vocab_size - 1\n",
    "eos_token_id = vocab_size - 1\n",
    "pad_token_id = vocab_size - 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [],
   "source": [
    "config = SmallTransformerConfig(\n",
    "    vocab_size=vocab_size,\n",
    "    n_embd=hidden_size,\n",
    "    n_layer=num_hidden_layers,\n",
    "    n_head=num_attention_heads,\n",
    "    # intermediate_size=intermediate_size,\n",
    "    # hidden_act=hidden_act,\n",
    "    # hidden_dropout_prob=hidden_dropout_prob,\n",
    "    # attention_probs_dropout_prob=attention_probs_dropout_prob,\n",
    "    n_positions=max_position_embeddings,\n",
    "    # type_vocab_size=type_vocab_size,\n",
    "    # initializer_range=initializer_range\n",
    "    bos_token_id=bos_token_id,\n",
    "    eos_token_id=eos_token_id,\n",
    "    pad_token_id=pad_token_id,\n",
    "    return_dict=True,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = TFSmallTransformerLMHeadModel(config=config)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "from transformers import Seq2SeqTrainingArguments, TFTrainer\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "using `logging_steps` to initialize `eval_steps` to 500\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/Users/hp/github/classes/cosc525/Implementation-Of-A-Lightweight-Transformer-And-Analysis-Of-Text-Generation-Sampling-Techniques/submodules/transformers/src/transformers/trainer_tf.py:115: FutureWarning: The class `TFTrainer` is deprecated and will be removed in version 5 of Transformers. We recommend using native Keras instead, by calling methods like `fit()` and `predict()` directly on the model object. Detailed examples of the Keras style can be found in our examples at https://github.com/huggingface/transformers/tree/main/examples/tensorflow\n",
      "  warnings.warn(\n",
      "You are instantiating a Trainer but W&B is not installed. To use wandb logging, run `pip install wandb && wandb login` see https://docs.wandb.com/huggingface.\n",
      "To use comet_ml logging, run `pip/conda install comet_ml` see https://www.comet.ml/docs/python-sdk/huggingface/\n"
     ]
    }
   ],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"small_transformer_trainer\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    # place_model_on_device=False,\n",
    ")\n",
    "metric = load_metric(\"accuracy\")\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "trainer = TFTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/opt/homebrew/Caskroom/miniforge/base/envs/huggingface/lib/python3.10/site-packages/keras/engine/training.py\", line 878, in train_function  *\n        return step_function(self, iterator)\n    File \"/opt/homebrew/Caskroom/miniforge/base/envs/huggingface/lib/python3.10/site-packages/keras/engine/training.py\", line 867, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/homebrew/Caskroom/miniforge/base/envs/huggingface/lib/python3.10/site-packages/keras/engine/training.py\", line 860, in run_step  **\n        outputs = model.train_step(data)\n    File \"/Users/hp/github/classes/cosc525/Implementation-Of-A-Lightweight-Transformer-And-Analysis-Of-Text-Generation-Sampling-Techniques/submodules/transformers/src/transformers/modeling_tf_utils.py\", line 1009, in train_step\n        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"/opt/homebrew/Caskroom/miniforge/base/envs/huggingface/lib/python3.10/site-packages/keras/optimizer_v2/optimizer_v2.py\", line 532, in minimize\n        return self.apply_gradients(grads_and_vars, name=name)\n    File \"/opt/homebrew/Caskroom/miniforge/base/envs/huggingface/lib/python3.10/site-packages/keras/optimizer_v2/optimizer_v2.py\", line 633, in apply_gradients\n        grads_and_vars = optimizer_utils.filter_empty_gradients(grads_and_vars)\n    File \"/opt/homebrew/Caskroom/miniforge/base/envs/huggingface/lib/python3.10/site-packages/keras/optimizer_v2/utils.py\", line 73, in filter_empty_gradients\n        raise ValueError(f\"No gradients provided for any variable: {variable}. \"\n\n    ValueError: No gradients provided for any variable: (['tf_small_transformer_lm_head_model/transformer/wpe/embeddings:0', 'tf_small_transformer_lm_head_model/transformer/wte/weight:0', 'tf_small_transformer_lm_head_model/transformer/h_._0/ln_1/gamma:0', 'tf_small_transformer_lm_head_model/transformer/h_._0/ln_1/beta:0', 'tf_small_transformer_lm_head_model/transformer/h_._0/attn/c_attn/weight:0', 'tf_small_transformer_lm_head_model/transformer/h_._0/attn/c_attn/bias:0', 'tf_small_transformer_lm_head_model/transformer/h_._0/attn/c_proj/weight:0', 'tf_small_transformer_lm_head_model/transformer/h_._0/attn/c_proj/bias:0', 'tf_small_transformer_lm_head_model/transformer/h_._0/ln_2/gamma:0', 'tf_small_transformer_lm_head_model/transformer/h_._0/ln_2/beta:0', 'tf_small_transformer_lm_head_model/transformer/h_._0/mlp/c_fc/weight:0', 'tf_small_transformer_lm_head_model/transformer/h_._0/mlp/c_fc/bias:0', 'tf_small_transformer_lm_head_model/transformer/h_._0/mlp/c_proj/weight:0', 'tf_small_transformer_lm_head_model/transformer/h_._0/mlp/c_proj/bias:0', 'tf_small_transformer_lm_head_model/transformer/h_._1/ln_1/gamma:0', 'tf_small_transformer_lm_head_model/transformer/h_._1/ln_1/beta:0', 'tf_small_transformer_lm_head_model/transformer/h_._1/attn/c_attn/weight:0', 'tf_small_transformer_lm_head_model/transformer/h_._1/attn/c_attn/bias:0', 'tf_small_transformer_lm_head_model/transformer/h_._1/attn/c_proj/weight:0', 'tf_small_transformer_lm_head_model/transformer/h_._1/attn/c_proj/bias:0', 'tf_small_transformer_lm_head_model/transformer/h_._1/ln_2/gamma:0', 'tf_small_transformer_lm_head_model/transformer/h_._1/ln_2/beta:0', 'tf_small_transformer_lm_head_model/transformer/h_._1/mlp/c_fc/weight:0', 'tf_small_transformer_lm_head_model/transformer/h_._1/mlp/c_fc/bias:0', 'tf_small_transformer_lm_head_model/transformer/h_._1/mlp/c_proj/weight:0', 'tf_small_transformer_lm_head_model/transformer/h_._1/mlp/c_proj/bias:0', 'tf_small_transformer_lm_head_model/transformer/h_._2/ln_1/gamma:0', 'tf_small_transformer_lm_head_model/transformer/h_._2/ln_1/beta:0', 'tf_small_transformer_lm_head_model/transformer/h_._2/attn/c_attn/weight:0', 'tf_small_transformer_lm_head_model/transformer/h_._2/attn/c_attn/bias:0', 'tf_small_transformer_lm_head_model/transformer/h_._2/attn/c_proj/weight:0', 'tf_small_transformer_lm_head_model/transformer/h_._2/attn/c_proj/bias:0', 'tf_small_transformer_lm_head_model/transformer/h_._2/ln_2/gamma:0', 'tf_small_transformer_lm_head_model/transformer/h_._2/ln_2/beta:0', 'tf_small_transformer_lm_head_model/transformer/h_._2/mlp/c_fc/weight:0', 'tf_small_transformer_lm_head_model/transformer/h_._2/mlp/c_fc/bias:0', 'tf_small_transformer_lm_head_model/transformer/h_._2/mlp/c_proj/weight:0', 'tf_small_transformer_lm_head_model/transformer/h_._2/mlp/c_proj/bias:0', 'tf_small_transformer_lm_head_model/transformer/h_._3/ln_1/gamma:0', 'tf_small_transformer_lm_head_model/transformer/h_._3/ln_1/beta:0', 'tf_small_transformer_lm_head_model/transformer/h_._3/attn/c_attn/weight:0', 'tf_small_transformer_lm_head_model/transformer/h_._3/attn/c_attn/bias:0', 'tf_small_transformer_lm_head_model/transformer/h_._3/attn/c_proj/weight:0', 'tf_small_transformer_lm_head_model/transformer/h_._3/attn/c_proj/bias:0', 'tf_small_transformer_lm_head_model/transformer/h_._3/ln_2/gamma:0', 'tf_small_transformer_lm_head_model/transformer/h_._3/ln_2/beta:0', 'tf_small_transformer_lm_head_model/transformer/h_._3/mlp/c_fc/weight:0', 'tf_small_transformer_lm_head_model/transformer/h_._3/mlp/c_fc/bias:0', 'tf_small_transformer_lm_head_model/transformer/h_._3/mlp/c_proj/weight:0', 'tf_small_transformer_lm_head_model/transformer/h_._3/mlp/c_proj/bias:0', 'tf_small_transformer_lm_head_model/transformer/h_._4/ln_1/gamma:0', 'tf_small_transformer_lm_head_model/transformer/h_._4/ln_1/beta:0', 'tf_small_transformer_lm_head_model/transformer/h_._4/attn/c_attn/weight:0', 'tf_small_transformer_lm_head_model/transformer/h_._4/attn/c_attn/bias:0', 'tf_small_transformer_lm_head_model/transformer/h_._4/attn/c_proj/weight:0', 'tf_small_transformer_lm_head_model/transformer/h_._4/attn/c_proj/bias:0', 'tf_small_transformer_lm_head_model/transformer/h_._4/ln_2/gamma:0', 'tf_small_transformer_lm_head_model/transformer/h_._4/ln_2/beta:0', 'tf_small_transformer_lm_head_model/transformer/h_._4/mlp/c_fc/weight:0', 'tf_small_transformer_lm_head_model/transformer/h_._4/mlp/c_fc/bias:0', 'tf_small_transformer_lm_head_model/transformer/h_._4/mlp/c_proj/weight:0', 'tf_small_transformer_lm_head_model/transformer/h_._4/mlp/c_proj/bias:0', 'tf_small_transformer_lm_head_model/transformer/ln_f/gamma:0', 'tf_small_transformer_lm_head_model/transformer/ln_f/beta:0'],). Provided `grads_and_vars` is ((None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/wpe/embeddings:0' shape=(512, 32) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/wte/weight:0' shape=(99, 32) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._0/ln_1/gamma:0' shape=(32,) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._0/ln_1/beta:0' shape=(32,) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._0/attn/c_attn/weight:0' shape=(32, 96) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._0/attn/c_attn/bias:0' shape=(1, 96) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._0/attn/c_proj/weight:0' shape=(32, 32) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._0/attn/c_proj/bias:0' shape=(1, 32) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._0/ln_2/gamma:0' shape=(32,) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._0/ln_2/beta:0' shape=(32,) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._0/mlp/c_fc/weight:0' shape=(32, 128) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._0/mlp/c_fc/bias:0' shape=(1, 128) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._0/mlp/c_proj/weight:0' shape=(128, 32) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._0/mlp/c_proj/bias:0' shape=(1, 32) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._1/ln_1/gamma:0' shape=(32,) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._1/ln_1/beta:0' shape=(32,) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._1/attn/c_attn/weight:0' shape=(32, 96) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._1/attn/c_attn/bias:0' shape=(1, 96) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._1/attn/c_proj/weight:0' shape=(32, 32) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._1/attn/c_proj/bias:0' shape=(1, 32) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._1/ln_2/gamma:0' shape=(32,) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._1/ln_2/beta:0' shape=(32,) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._1/mlp/c_fc/weight:0' shape=(32, 128) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._1/mlp/c_fc/bias:0' shape=(1, 128) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._1/mlp/c_proj/weight:0' shape=(128, 32) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._1/mlp/c_proj/bias:0' shape=(1, 32) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._2/ln_1/gamma:0' shape=(32,) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._2/ln_1/beta:0' shape=(32,) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._2/attn/c_attn/weight:0' shape=(32, 96) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._2/attn/c_attn/bias:0' shape=(1, 96) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._2/attn/c_proj/weight:0' shape=(32, 32) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._2/attn/c_proj/bias:0' shape=(1, 32) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._2/ln_2/gamma:0' shape=(32,) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._2/ln_2/beta:0' shape=(32,) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._2/mlp/c_fc/weight:0' shape=(32, 128) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._2/mlp/c_fc/bias:0' shape=(1, 128) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._2/mlp/c_proj/weight:0' shape=(128, 32) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._2/mlp/c_proj/bias:0' shape=(1, 32) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._3/ln_1/gamma:0' shape=(32,) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._3/ln_1/beta:0' shape=(32,) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._3/attn/c_attn/weight:0' shape=(32, 96) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._3/attn/c_attn/bias:0' shape=(1, 96) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._3/attn/c_proj/weight:0' shape=(32, 32) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._3/attn/c_proj/bias:0' shape=(1, 32) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._3/ln_2/gamma:0' shape=(32,) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._3/ln_2/beta:0' shape=(32,) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._3/mlp/c_fc/weight:0' shape=(32, 128) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._3/mlp/c_fc/bias:0' shape=(1, 128) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._3/mlp/c_proj/weight:0' shape=(128, 32) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._3/mlp/c_proj/bias:0' shape=(1, 32) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._4/ln_1/gamma:0' shape=(32,) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._4/ln_1/beta:0' shape=(32,) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._4/attn/c_attn/weight:0' shape=(32, 96) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._4/attn/c_attn/bias:0' shape=(1, 96) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._4/attn/c_proj/weight:0' shape=(32, 32) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._4/attn/c_proj/bias:0' shape=(1, 32) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._4/ln_2/gamma:0' shape=(32,) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._4/ln_2/beta:0' shape=(32,) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._4/mlp/c_fc/weight:0' shape=(32, 128) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._4/mlp/c_fc/bias:0' shape=(1, 128) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._4/mlp/c_proj/weight:0' shape=(128, 32) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._4/mlp/c_proj/bias:0' shape=(1, 32) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/ln_f/gamma:0' shape=(32,) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/ln_f/beta:0' shape=(32,) dtype=float32>)).\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[0;32mIn [106]\u001B[0m, in \u001B[0;36m<cell line: 2>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m model\u001B[38;5;241m.\u001B[39mcompile(optimizer\u001B[38;5;241m=\u001B[39mtf\u001B[38;5;241m.\u001B[39mkeras\u001B[38;5;241m.\u001B[39moptimizers\u001B[38;5;241m.\u001B[39mAdam(learning_rate\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.001\u001B[39m), loss\u001B[38;5;241m=\u001B[39mtf\u001B[38;5;241m.\u001B[39mkeras\u001B[38;5;241m.\u001B[39mlosses\u001B[38;5;241m.\u001B[39mSparseCategoricalCrossentropy(from_logits\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m))\n\u001B[0;32m----> 2\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtest_dataset\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/huggingface/lib/python3.10/site-packages/keras/utils/traceback_utils.py:67\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     65\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:  \u001B[38;5;66;03m# pylint: disable=broad-except\u001B[39;00m\n\u001B[1;32m     66\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[0;32m---> 67\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m     68\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m     69\u001B[0m   \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/huggingface/lib/python3.10/site-packages/tensorflow/python/framework/func_graph.py:1129\u001B[0m, in \u001B[0;36mfunc_graph_from_py_func.<locals>.autograph_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m   1127\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:  \u001B[38;5;66;03m# pylint:disable=broad-except\u001B[39;00m\n\u001B[1;32m   1128\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(e, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mag_error_metadata\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m-> 1129\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mag_error_metadata\u001B[38;5;241m.\u001B[39mto_exception(e)\n\u001B[1;32m   1130\u001B[0m   \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1131\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
      "\u001B[0;31mValueError\u001B[0m: in user code:\n\n    File \"/opt/homebrew/Caskroom/miniforge/base/envs/huggingface/lib/python3.10/site-packages/keras/engine/training.py\", line 878, in train_function  *\n        return step_function(self, iterator)\n    File \"/opt/homebrew/Caskroom/miniforge/base/envs/huggingface/lib/python3.10/site-packages/keras/engine/training.py\", line 867, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/homebrew/Caskroom/miniforge/base/envs/huggingface/lib/python3.10/site-packages/keras/engine/training.py\", line 860, in run_step  **\n        outputs = model.train_step(data)\n    File \"/Users/hp/github/classes/cosc525/Implementation-Of-A-Lightweight-Transformer-And-Analysis-Of-Text-Generation-Sampling-Techniques/submodules/transformers/src/transformers/modeling_tf_utils.py\", line 1009, in train_step\n        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"/opt/homebrew/Caskroom/miniforge/base/envs/huggingface/lib/python3.10/site-packages/keras/optimizer_v2/optimizer_v2.py\", line 532, in minimize\n        return self.apply_gradients(grads_and_vars, name=name)\n    File \"/opt/homebrew/Caskroom/miniforge/base/envs/huggingface/lib/python3.10/site-packages/keras/optimizer_v2/optimizer_v2.py\", line 633, in apply_gradients\n        grads_and_vars = optimizer_utils.filter_empty_gradients(grads_and_vars)\n    File \"/opt/homebrew/Caskroom/miniforge/base/envs/huggingface/lib/python3.10/site-packages/keras/optimizer_v2/utils.py\", line 73, in filter_empty_gradients\n        raise ValueError(f\"No gradients provided for any variable: {variable}. \"\n\n    ValueError: No gradients provided for any variable: (['tf_small_transformer_lm_head_model/transformer/wpe/embeddings:0', 'tf_small_transformer_lm_head_model/transformer/wte/weight:0', 'tf_small_transformer_lm_head_model/transformer/h_._0/ln_1/gamma:0', 'tf_small_transformer_lm_head_model/transformer/h_._0/ln_1/beta:0', 'tf_small_transformer_lm_head_model/transformer/h_._0/attn/c_attn/weight:0', 'tf_small_transformer_lm_head_model/transformer/h_._0/attn/c_attn/bias:0', 'tf_small_transformer_lm_head_model/transformer/h_._0/attn/c_proj/weight:0', 'tf_small_transformer_lm_head_model/transformer/h_._0/attn/c_proj/bias:0', 'tf_small_transformer_lm_head_model/transformer/h_._0/ln_2/gamma:0', 'tf_small_transformer_lm_head_model/transformer/h_._0/ln_2/beta:0', 'tf_small_transformer_lm_head_model/transformer/h_._0/mlp/c_fc/weight:0', 'tf_small_transformer_lm_head_model/transformer/h_._0/mlp/c_fc/bias:0', 'tf_small_transformer_lm_head_model/transformer/h_._0/mlp/c_proj/weight:0', 'tf_small_transformer_lm_head_model/transformer/h_._0/mlp/c_proj/bias:0', 'tf_small_transformer_lm_head_model/transformer/h_._1/ln_1/gamma:0', 'tf_small_transformer_lm_head_model/transformer/h_._1/ln_1/beta:0', 'tf_small_transformer_lm_head_model/transformer/h_._1/attn/c_attn/weight:0', 'tf_small_transformer_lm_head_model/transformer/h_._1/attn/c_attn/bias:0', 'tf_small_transformer_lm_head_model/transformer/h_._1/attn/c_proj/weight:0', 'tf_small_transformer_lm_head_model/transformer/h_._1/attn/c_proj/bias:0', 'tf_small_transformer_lm_head_model/transformer/h_._1/ln_2/gamma:0', 'tf_small_transformer_lm_head_model/transformer/h_._1/ln_2/beta:0', 'tf_small_transformer_lm_head_model/transformer/h_._1/mlp/c_fc/weight:0', 'tf_small_transformer_lm_head_model/transformer/h_._1/mlp/c_fc/bias:0', 'tf_small_transformer_lm_head_model/transformer/h_._1/mlp/c_proj/weight:0', 'tf_small_transformer_lm_head_model/transformer/h_._1/mlp/c_proj/bias:0', 'tf_small_transformer_lm_head_model/transformer/h_._2/ln_1/gamma:0', 'tf_small_transformer_lm_head_model/transformer/h_._2/ln_1/beta:0', 'tf_small_transformer_lm_head_model/transformer/h_._2/attn/c_attn/weight:0', 'tf_small_transformer_lm_head_model/transformer/h_._2/attn/c_attn/bias:0', 'tf_small_transformer_lm_head_model/transformer/h_._2/attn/c_proj/weight:0', 'tf_small_transformer_lm_head_model/transformer/h_._2/attn/c_proj/bias:0', 'tf_small_transformer_lm_head_model/transformer/h_._2/ln_2/gamma:0', 'tf_small_transformer_lm_head_model/transformer/h_._2/ln_2/beta:0', 'tf_small_transformer_lm_head_model/transformer/h_._2/mlp/c_fc/weight:0', 'tf_small_transformer_lm_head_model/transformer/h_._2/mlp/c_fc/bias:0', 'tf_small_transformer_lm_head_model/transformer/h_._2/mlp/c_proj/weight:0', 'tf_small_transformer_lm_head_model/transformer/h_._2/mlp/c_proj/bias:0', 'tf_small_transformer_lm_head_model/transformer/h_._3/ln_1/gamma:0', 'tf_small_transformer_lm_head_model/transformer/h_._3/ln_1/beta:0', 'tf_small_transformer_lm_head_model/transformer/h_._3/attn/c_attn/weight:0', 'tf_small_transformer_lm_head_model/transformer/h_._3/attn/c_attn/bias:0', 'tf_small_transformer_lm_head_model/transformer/h_._3/attn/c_proj/weight:0', 'tf_small_transformer_lm_head_model/transformer/h_._3/attn/c_proj/bias:0', 'tf_small_transformer_lm_head_model/transformer/h_._3/ln_2/gamma:0', 'tf_small_transformer_lm_head_model/transformer/h_._3/ln_2/beta:0', 'tf_small_transformer_lm_head_model/transformer/h_._3/mlp/c_fc/weight:0', 'tf_small_transformer_lm_head_model/transformer/h_._3/mlp/c_fc/bias:0', 'tf_small_transformer_lm_head_model/transformer/h_._3/mlp/c_proj/weight:0', 'tf_small_transformer_lm_head_model/transformer/h_._3/mlp/c_proj/bias:0', 'tf_small_transformer_lm_head_model/transformer/h_._4/ln_1/gamma:0', 'tf_small_transformer_lm_head_model/transformer/h_._4/ln_1/beta:0', 'tf_small_transformer_lm_head_model/transformer/h_._4/attn/c_attn/weight:0', 'tf_small_transformer_lm_head_model/transformer/h_._4/attn/c_attn/bias:0', 'tf_small_transformer_lm_head_model/transformer/h_._4/attn/c_proj/weight:0', 'tf_small_transformer_lm_head_model/transformer/h_._4/attn/c_proj/bias:0', 'tf_small_transformer_lm_head_model/transformer/h_._4/ln_2/gamma:0', 'tf_small_transformer_lm_head_model/transformer/h_._4/ln_2/beta:0', 'tf_small_transformer_lm_head_model/transformer/h_._4/mlp/c_fc/weight:0', 'tf_small_transformer_lm_head_model/transformer/h_._4/mlp/c_fc/bias:0', 'tf_small_transformer_lm_head_model/transformer/h_._4/mlp/c_proj/weight:0', 'tf_small_transformer_lm_head_model/transformer/h_._4/mlp/c_proj/bias:0', 'tf_small_transformer_lm_head_model/transformer/ln_f/gamma:0', 'tf_small_transformer_lm_head_model/transformer/ln_f/beta:0'],). Provided `grads_and_vars` is ((None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/wpe/embeddings:0' shape=(512, 32) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/wte/weight:0' shape=(99, 32) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._0/ln_1/gamma:0' shape=(32,) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._0/ln_1/beta:0' shape=(32,) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._0/attn/c_attn/weight:0' shape=(32, 96) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._0/attn/c_attn/bias:0' shape=(1, 96) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._0/attn/c_proj/weight:0' shape=(32, 32) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._0/attn/c_proj/bias:0' shape=(1, 32) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._0/ln_2/gamma:0' shape=(32,) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._0/ln_2/beta:0' shape=(32,) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._0/mlp/c_fc/weight:0' shape=(32, 128) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._0/mlp/c_fc/bias:0' shape=(1, 128) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._0/mlp/c_proj/weight:0' shape=(128, 32) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._0/mlp/c_proj/bias:0' shape=(1, 32) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._1/ln_1/gamma:0' shape=(32,) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._1/ln_1/beta:0' shape=(32,) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._1/attn/c_attn/weight:0' shape=(32, 96) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._1/attn/c_attn/bias:0' shape=(1, 96) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._1/attn/c_proj/weight:0' shape=(32, 32) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._1/attn/c_proj/bias:0' shape=(1, 32) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._1/ln_2/gamma:0' shape=(32,) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._1/ln_2/beta:0' shape=(32,) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._1/mlp/c_fc/weight:0' shape=(32, 128) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._1/mlp/c_fc/bias:0' shape=(1, 128) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._1/mlp/c_proj/weight:0' shape=(128, 32) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._1/mlp/c_proj/bias:0' shape=(1, 32) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._2/ln_1/gamma:0' shape=(32,) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._2/ln_1/beta:0' shape=(32,) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._2/attn/c_attn/weight:0' shape=(32, 96) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._2/attn/c_attn/bias:0' shape=(1, 96) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._2/attn/c_proj/weight:0' shape=(32, 32) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._2/attn/c_proj/bias:0' shape=(1, 32) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._2/ln_2/gamma:0' shape=(32,) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._2/ln_2/beta:0' shape=(32,) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._2/mlp/c_fc/weight:0' shape=(32, 128) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._2/mlp/c_fc/bias:0' shape=(1, 128) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._2/mlp/c_proj/weight:0' shape=(128, 32) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._2/mlp/c_proj/bias:0' shape=(1, 32) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._3/ln_1/gamma:0' shape=(32,) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._3/ln_1/beta:0' shape=(32,) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._3/attn/c_attn/weight:0' shape=(32, 96) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._3/attn/c_attn/bias:0' shape=(1, 96) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._3/attn/c_proj/weight:0' shape=(32, 32) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._3/attn/c_proj/bias:0' shape=(1, 32) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._3/ln_2/gamma:0' shape=(32,) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._3/ln_2/beta:0' shape=(32,) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._3/mlp/c_fc/weight:0' shape=(32, 128) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._3/mlp/c_fc/bias:0' shape=(1, 128) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._3/mlp/c_proj/weight:0' shape=(128, 32) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._3/mlp/c_proj/bias:0' shape=(1, 32) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._4/ln_1/gamma:0' shape=(32,) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._4/ln_1/beta:0' shape=(32,) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._4/attn/c_attn/weight:0' shape=(32, 96) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._4/attn/c_attn/bias:0' shape=(1, 96) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._4/attn/c_proj/weight:0' shape=(32, 32) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._4/attn/c_proj/bias:0' shape=(1, 32) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._4/ln_2/gamma:0' shape=(32,) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._4/ln_2/beta:0' shape=(32,) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._4/mlp/c_fc/weight:0' shape=(32, 128) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._4/mlp/c_fc/bias:0' shape=(1, 128) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._4/mlp/c_proj/weight:0' shape=(128, 32) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/h_._4/mlp/c_proj/bias:0' shape=(1, 32) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/ln_f/gamma:0' shape=(32,) dtype=float32>), (None, <tf.Variable 'tf_small_transformer_lm_head_model/transformer/ln_f/beta:0' shape=(32,) dtype=float32>)).\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))\n",
    "model.fit(test_dataset)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'PrefetchDataset' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[0;32mIn [87]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m model(\u001B[43mtrain_dataset\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43minput_ids\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m, train_dataset[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mattention_mask\u001B[39m\u001B[38;5;124m'\u001B[39m], training\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[0;31mTypeError\u001B[0m: 'PrefetchDataset' object is not subscriptable"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "0: I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with my dog. I'm not sure if I'll ever be able to walk with my dog.\n",
      "\n",
      "I'm not sure if I'll\n"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}