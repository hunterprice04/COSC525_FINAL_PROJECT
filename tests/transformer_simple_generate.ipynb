{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# @formatter:off\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# @formatter:on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wPNlg_uDfCP1",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "M_VOCAB_SZ = 20000\n",
    "M_MAX_LEN = 80\n",
    "M_ATT_HEADS = 2\n",
    "M_DIM_EMB = 256\n",
    "M_DIM_FFN = 256\n",
    "M_WARMUP_STEPS = 2000\n",
    "T_BATCH_SIZE = 256\n",
    "T_EPOCHS = 5\n",
    "\n",
    "data_files = [\"data_test/rap.txt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N0aQqXlMsGVb",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def causal_attention_mask(batch_size, n_dest, n_src, dtype):\n",
    "    i = tf.range(n_dest)[:, None]\n",
    "    j = tf.range(n_src)\n",
    "    m = i >= j - n_src + n_dest\n",
    "    mask = tf.cast(m, dtype)\n",
    "    mask = tf.reshape(mask, [1, n_dest, n_src])\n",
    "    mult = tf.concat(\n",
    "        [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], 0\n",
    "    )\n",
    "    return tf.tile(mask, mult)\n",
    "\n",
    "\n",
    "class Transformer(layers.Layer):\n",
    "    def __init__(self, embedding_dim, num_att_heads, state_dims, dropout_rate=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_att_heads = num_att_heads\n",
    "        self.state_dims = state_dims\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.attention = layers.MultiHeadAttention(num_att_heads, embedding_dim)\n",
    "        self.feed_forward = keras.Sequential(\n",
    "            [layers.Dense(state_dims, activation=\"relu\"), layers.Dense(embedding_dim)]\n",
    "        )\n",
    "        self.norm1, self.norm2 = layers.LayerNormalization(\n",
    "            epsilon=1e-6\n",
    "        ), layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.drop1, self.drop2 = layers.Dropout(dropout_rate), layers.Dropout(\n",
    "            dropout_rate\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        inp_shape = tf.shape(inputs)\n",
    "        batch_sz, seq_len = inp_shape[0], inp_shape[1]\n",
    "        causal_mask = causal_attention_mask(batch_sz, seq_len, seq_len, tf.bool)\n",
    "        attention_out = self.attention(inputs, inputs, attention_mask=causal_mask)\n",
    "        attention_out = self.drop1(attention_out)\n",
    "        out1 = self.norm1(inputs + attention_out)\n",
    "        feed_forward_out = self.feed_forward(out1)\n",
    "        feed_forward_out = self.drop2(feed_forward_out)\n",
    "        return self.norm2(out1 + feed_forward_out)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"embedding_dim\": self.embedding_dim,\n",
    "                \"num_att_heads\": self.num_att_heads,\n",
    "                \"state_dims\": self.state_dims,\n",
    "                \"dropout_rate\": self.dropout_rate,\n",
    "            }\n",
    "        )\n",
    "        return config\n",
    "\n",
    "\n",
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, max_len, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.max_len = max_len\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=max_len, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        max_len = tf.shape(x)[-1]\n",
    "        pos = tf.range(start=0, limit=max_len, delta=1)\n",
    "        pos = self.pos_emb(pos)\n",
    "        x = self.token_emb(x)\n",
    "        return x + pos\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"max_len\": self.max_len,\n",
    "                \"vocab_size\": self.vocab_size,\n",
    "                \"embed_dim\": self.embed_dim,\n",
    "            }\n",
    "        )\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class WarmupScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, embedding_dim, warmup_steps):\n",
    "        super(WarmupScheduler, self).__init__()\n",
    "        self.emb_dim = tf.cast(embedding_dim, tf.float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, tf.float32)\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps**-1.5)\n",
    "        lr = tf.math.rsqrt(self.emb_dim) * tf.math.minimum(arg1, arg2)\n",
    "        # print(f\"WarmupScheduler: {step} => {lr}\")\n",
    "        return lr\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"emb_dim\": self.emb_dim,\n",
    "                \"warmup_steps\": self.warmup_steps,\n",
    "            }\n",
    "        )\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    # TODO: Remove this if we're using tokenizer\n",
    "    embedding = TokenAndPositionEmbedding(M_MAX_LEN, M_VOCAB_SZ, M_DIM_EMB)\n",
    "    transformer = Transformer(M_DIM_EMB, M_ATT_HEADS, M_DIM_FFN)\n",
    "\n",
    "    l_input = layers.Input(shape=(M_MAX_LEN,), dtype=tf.int32)\n",
    "    l_emb = embedding(l_input)\n",
    "    l_trans = transformer(l_emb)\n",
    "    l_output = layers.Dense(M_VOCAB_SZ)(l_trans)\n",
    "\n",
    "    m = keras.Model(inputs=l_input, outputs=[l_output, l_trans])\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    learning_rate = WarmupScheduler(M_DIM_EMB, M_WARMUP_STEPS)\n",
    "    optimizer = tf.keras.optimizers.Adam(\n",
    "        learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9\n",
    "    )\n",
    "\n",
    "    m.compile(optimizer, loss=[loss_fn, None])\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def create_dataset(file_pth, batch_sz, buf_sz=1000, shuffle=True):\n",
    "    # Shuffle the data and create batches\n",
    "    if shuffle:\n",
    "        random.shuffle(file_pth)\n",
    "    ds = tf.data.TextLineDataset(file_pth)\n",
    "    ds = ds.shuffle(buffer_size=buf_sz)\n",
    "    ds = ds.batch(batch_sz)\n",
    "    return ds\n",
    "\n",
    "\n",
    "def create_tokenizer(dataset, max_vocab_size, max_seq_len):\n",
    "    def preprocess_txt(input_string):\n",
    "        # Preprocessing for word-level model\n",
    "        s1 = tf.strings.lower(input_string)\n",
    "        return tf.strings.regex_replace(s1, f\"([{string.punctuation}])\", r\" \\1\")\n",
    "\n",
    "    # Vectorization of the data\n",
    "    vectorize = TextVectorization(\n",
    "        standardize=preprocess_txt,\n",
    "        max_tokens=max_vocab_size - 1,\n",
    "        output_mode=\"int\",\n",
    "        output_sequence_length=max_seq_len + 1,\n",
    "    )\n",
    "    vectorize.adapt(dataset)\n",
    "    vocab = vectorize.get_vocabulary()\n",
    "    return vectorize, vocab\n",
    "\n",
    "\n",
    "# Read in the data and create the dataset\n",
    "dataset = create_dataset(data_files, T_BATCH_SIZE)\n",
    "# Create the tokenizer\n",
    "tokenizer, vocab = create_tokenizer(dataset, M_VOCAB_SZ, M_MAX_LEN)\n",
    "\n",
    "\n",
    "def create_sequences(txt):\n",
    "    txt = tf.expand_dims(txt, -1)\n",
    "    txt_tok = tokenizer(txt)\n",
    "    return txt_tok[:, :-1], txt_tok[:, 1:]\n",
    "\n",
    "\n",
    "dataset = dataset.map(create_sequences).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(256, 80), dtype=int64, numpy=\n",
      "array([[ 114,   15,   46, ...,    0,    0,    0],\n",
      "       [ 506, 5039, 5250, ...,    0,    0,    0],\n",
      "       [  95,    4, 3806, ...,    0,    0,    0],\n",
      "       ...,\n",
      "       [  49,  610,   26, ...,    0,    0,    0],\n",
      "       [ 166,   58,    4, ...,    0,    0,    0],\n",
      "       [ 326,    6, 5727, ...,    0,    0,    0]])>, <tf.Tensor: shape=(256, 80), dtype=int64, numpy=\n",
      "array([[  15,   46,  143, ...,    0,    0,    0],\n",
      "       [5039, 5250,  491, ...,    0,    0,    0],\n",
      "       [   4, 3806,  762, ...,    0,    0,    0],\n",
      "       ...,\n",
      "       [ 610,   26,  194, ...,    0,    0,    0],\n",
      "       [  58,    4,    1, ...,    0,    0,    0],\n",
      "       [   6, 5727, 1817, ...,    0,    0,    0]])>)\n"
     ]
    }
   ],
   "source": [
    "for d in dataset.take(1):\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class TextGenerator(keras.callbacks.Callback):\n",
    "    def __init__(\n",
    "        self, max_tokens, start_tokens, index_to_word, top_k=10, print_every=1\n",
    "    ):\n",
    "        self.max_tokens = max_tokens\n",
    "        self.start_tokens = start_tokens\n",
    "        self.index_to_word = index_to_word\n",
    "        self.print_every = print_every\n",
    "        self.k = top_k\n",
    "\n",
    "    def sample_top_k(self, logits):\n",
    "        logits, indices = tf.math.top_k(logits, k=self.k, sorted=True)\n",
    "        indices = np.asarray(indices).astype(\"int32\")\n",
    "        preds = keras.activations.softmax(tf.expand_dims(logits, 0))[0]\n",
    "        preds = np.asarray(preds).astype(\"float32\")\n",
    "        return np.random.choice(indices, p=preds)\n",
    "\n",
    "    def detokenize(self, number):\n",
    "        return self.index_to_word[number]\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        start_tokens = list(self.start_tokens)\n",
    "        if (epoch + 1) % self.print_every != 0:\n",
    "            return\n",
    "        num_tokens_generated = 0\n",
    "        tokens_generated = []\n",
    "        while num_tokens_generated <= self.max_tokens:\n",
    "            pad_len = M_MAX_LEN - len(start_tokens)\n",
    "            sample_index = len(start_tokens) - 1\n",
    "            if pad_len < 0:\n",
    "                x = start_tokens[:M_MAX_LEN]\n",
    "                sample_index = M_MAX_LEN - 1\n",
    "            elif pad_len > 0:\n",
    "                x = start_tokens + [0] * pad_len\n",
    "            else:\n",
    "                x = start_tokens\n",
    "            x = np.array([x])\n",
    "            y, _ = self.model.predict(x)\n",
    "            sample_token = self.sample_top_k(y[0][sample_index])\n",
    "            tokens_generated.append(sample_token)\n",
    "            start_tokens.append(sample_token)\n",
    "            num_tokens_generated = len(tokens_generated)\n",
    "        txt = \" \".join(\n",
    "            [self.detokenize(_) for _ in self.start_tokens + tokens_generated]\n",
    "        )\n",
    "        print(f\"Generated:\\n{txt}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def create_generation_callback(start_prompt, vocabulary, gen_len=100):\n",
    "    # Tokenize starting prompt\n",
    "    word_to_index = {word: index for index, word in enumerate(vocabulary)}\n",
    "    prompt_tokens = [word_to_index.get(_, 1) for _ in start_prompt.lower().split()]\n",
    "    return TextGenerator(gen_len, prompt_tokens, vocabulary)\n",
    "\n",
    "\n",
    "def create_callbacks(base_dir, model, defaults: list = None):\n",
    "    dir_models = os.path.join(base_dir, model.name)\n",
    "    os.makedirs(dir_models, exist_ok=True)\n",
    "    print(f\"# Model Dir: {dir_models}\")\n",
    "\n",
    "    path_csv = os.path.join(dir_models, \"history.csv\")\n",
    "    print(\" - History CSV:\", path_csv)\n",
    "\n",
    "    path_ckp = os.path.join(dir_models, \"checkpoints.h5\")\n",
    "    print(\" - Checkpoint:\", path_ckp)\n",
    "\n",
    "    path_tb = os.path.join(dir_models, \"logs\")\n",
    "    tb_file_writer = tf.summary.create_file_writer(path_tb)\n",
    "    callbacks = [] if defaults is None else defaults\n",
    "    callbacks.append(tf.keras.callbacks.TensorBoard(log_dir=path_tb, histogram_freq=1))\n",
    "    callbacks.append(tf.keras.callbacks.CSVLogger(path_csv, separator=\",\", append=True))\n",
    "    # callbacks.append(tf.keras.callbacks.ModelCheckpoint(path_ckp,\n",
    "    #                                                     monitor='loss',\n",
    "    #                                                     save_best_only=True,\n",
    "    #                                                     mode='auto',\n",
    "    #                                                     verbose=0))\n",
    "    os.makedirs(dir_models, exist_ok=True)\n",
    "    return callbacks, tb_file_writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Model Dir: logs/model_1\n",
      " - History CSV: logs/model_1/history.csv\n",
      " - Checkpoint: logs/model_1/checkpoints.h5\n"
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "callbacks, tb_file_writer = create_callbacks(\"logs\", model)\n",
    "gen_callback = create_generation_callback(\"a day in the life\", vocab)\n",
    "callbacks.append(gen_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-16 23:18:28.608855: E tensorflow/stream_executor/cuda/cuda_blas.cc:232] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\n",
      "2022-05-16 23:18:28.608889: E tensorflow/stream_executor/cuda/cuda_blas.cc:234] Failure to initialize cublas may be due to OOM (cublas needs some free memory when you initialize it, and your deep-learning framework may have preallocated more than its fair share), or may be because this binary was not built with support for the GPU in your machine.\n",
      "2022-05-16 23:18:28.608916: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at matmul_op_impl.h:442 : INTERNAL: Attempting to perform BLAS operation using StreamExecutor without BLAS support\n",
      "2022-05-16 23:18:28.611674: E tensorflow/stream_executor/cuda/cuda_blas.cc:232] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\n",
      "2022-05-16 23:18:28.611690: E tensorflow/stream_executor/cuda/cuda_blas.cc:234] Failure to initialize cublas may be due to OOM (cublas needs some free memory when you initialize it, and your deep-learning framework may have preallocated more than its fair share), or may be because this binary was not built with support for the GPU in your machine.\n",
      "2022-05-16 23:18:28.611700: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at matmul_op_impl.h:442 : INTERNAL: Attempting to perform BLAS operation using StreamExecutor without BLAS support\n",
      "2022-05-16 23:18:28.613907: E tensorflow/stream_executor/cuda/cuda_blas.cc:232] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\n",
      "2022-05-16 23:18:28.613922: E tensorflow/stream_executor/cuda/cuda_blas.cc:234] Failure to initialize cublas may be due to OOM (cublas needs some free memory when you initialize it, and your deep-learning framework may have preallocated more than its fair share), or may be because this binary was not built with support for the GPU in your machine.\n",
      "2022-05-16 23:18:28.613931: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at matmul_op_impl.h:442 : INTERNAL: Attempting to perform BLAS operation using StreamExecutor without BLAS support\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Graph execution error:\n\nDetected at node 'model/transformer/multi_head_attention/key/einsum/Einsum' defined at (most recent call last):\n    File \"/opt/conda/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/opt/conda/lib/python3.9/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/opt/conda/lib/python3.9/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/opt/conda/lib/python3.9/site-packages/traitlets/config/application.py\", line 846, in launch_instance\n      app.start()\n    File \"/opt/conda/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"/opt/conda/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"/opt/conda/lib/python3.9/asyncio/base_events.py\", line 596, in run_forever\n      self._run_once()\n    File \"/opt/conda/lib/python3.9/asyncio/base_events.py\", line 1890, in _run_once\n      handle._run()\n    File \"/opt/conda/lib/python3.9/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/opt/conda/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 504, in dispatch_queue\n      await self.process_one()\n    File \"/opt/conda/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 493, in process_one\n      await dispatch(*args)\n    File \"/opt/conda/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 400, in dispatch_shell\n      await result\n    File \"/opt/conda/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 724, in execute_request\n      reply_content = await reply_content\n    File \"/opt/conda/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 390, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"/opt/conda/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/opt/conda/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2863, in run_cell\n      result = self._run_cell(\n    File \"/opt/conda/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2909, in _run_cell\n      return runner(coro)\n    File \"/opt/conda/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/opt/conda/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3106, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/opt/conda/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3309, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/opt/conda/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3369, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_2023/405319210.py\", line 3, in <cell line: 3>\n      model.fit(dataset, verbose=2, epochs=T_EPOCHS, callbacks=callbacks)\n    File \"/opt/conda/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/opt/conda/lib/python3.9/site-packages/keras/engine/training.py\", line 1384, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/opt/conda/lib/python3.9/site-packages/keras/engine/training.py\", line 1021, in train_function\n      return step_function(self, iterator)\n    File \"/opt/conda/lib/python3.9/site-packages/keras/engine/training.py\", line 1010, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/conda/lib/python3.9/site-packages/keras/engine/training.py\", line 1000, in run_step\n      outputs = model.train_step(data)\n    File \"/opt/conda/lib/python3.9/site-packages/keras/engine/training.py\", line 859, in train_step\n      y_pred = self(x, training=True)\n    File \"/opt/conda/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/opt/conda/lib/python3.9/site-packages/keras/engine/base_layer.py\", line 1096, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/opt/conda/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"/opt/conda/lib/python3.9/site-packages/keras/engine/functional.py\", line 451, in call\n      return self._run_internal_graph(\n    File \"/opt/conda/lib/python3.9/site-packages/keras/engine/functional.py\", line 589, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"/opt/conda/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/opt/conda/lib/python3.9/site-packages/keras/engine/base_layer.py\", line 1096, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/opt/conda/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"/tmp/ipykernel_2023/3326115625.py\", line 30, in call\n      attention_out = self.attention(inputs, inputs, attention_mask=causal_mask)\n    File \"/opt/conda/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/opt/conda/lib/python3.9/site-packages/keras/engine/base_layer.py\", line 1096, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/opt/conda/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"/opt/conda/lib/python3.9/site-packages/keras/layers/multi_head_attention.py\", line 505, in call\n      key = self._key_dense(key)\n    File \"/opt/conda/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/opt/conda/lib/python3.9/site-packages/keras/engine/base_layer.py\", line 1096, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/opt/conda/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"/opt/conda/lib/python3.9/site-packages/keras/layers/einsum_dense.py\", line 187, in call\n      ret = tf.einsum(self.equation, inputs, self.kernel)\nNode: 'model/transformer/multi_head_attention/key/einsum/Einsum'\nAttempting to perform BLAS operation using StreamExecutor without BLAS support\n\t [[{{node model/transformer/multi_head_attention/key/einsum/Einsum}}]] [Op:__inference_train_function_2840]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# wandb.tensorboard.patch(root_logdir=\"logs\")\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# wandb.init(project='transformer')\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mT_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mInternalError\u001b[0m: Graph execution error:\n\nDetected at node 'model/transformer/multi_head_attention/key/einsum/Einsum' defined at (most recent call last):\n    File \"/opt/conda/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/opt/conda/lib/python3.9/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/opt/conda/lib/python3.9/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/opt/conda/lib/python3.9/site-packages/traitlets/config/application.py\", line 846, in launch_instance\n      app.start()\n    File \"/opt/conda/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"/opt/conda/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"/opt/conda/lib/python3.9/asyncio/base_events.py\", line 596, in run_forever\n      self._run_once()\n    File \"/opt/conda/lib/python3.9/asyncio/base_events.py\", line 1890, in _run_once\n      handle._run()\n    File \"/opt/conda/lib/python3.9/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/opt/conda/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 504, in dispatch_queue\n      await self.process_one()\n    File \"/opt/conda/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 493, in process_one\n      await dispatch(*args)\n    File \"/opt/conda/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 400, in dispatch_shell\n      await result\n    File \"/opt/conda/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 724, in execute_request\n      reply_content = await reply_content\n    File \"/opt/conda/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 390, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"/opt/conda/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/opt/conda/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2863, in run_cell\n      result = self._run_cell(\n    File \"/opt/conda/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2909, in _run_cell\n      return runner(coro)\n    File \"/opt/conda/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/opt/conda/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3106, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/opt/conda/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3309, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/opt/conda/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3369, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_2023/405319210.py\", line 3, in <cell line: 3>\n      model.fit(dataset, verbose=2, epochs=T_EPOCHS, callbacks=callbacks)\n    File \"/opt/conda/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/opt/conda/lib/python3.9/site-packages/keras/engine/training.py\", line 1384, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/opt/conda/lib/python3.9/site-packages/keras/engine/training.py\", line 1021, in train_function\n      return step_function(self, iterator)\n    File \"/opt/conda/lib/python3.9/site-packages/keras/engine/training.py\", line 1010, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/conda/lib/python3.9/site-packages/keras/engine/training.py\", line 1000, in run_step\n      outputs = model.train_step(data)\n    File \"/opt/conda/lib/python3.9/site-packages/keras/engine/training.py\", line 859, in train_step\n      y_pred = self(x, training=True)\n    File \"/opt/conda/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/opt/conda/lib/python3.9/site-packages/keras/engine/base_layer.py\", line 1096, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/opt/conda/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"/opt/conda/lib/python3.9/site-packages/keras/engine/functional.py\", line 451, in call\n      return self._run_internal_graph(\n    File \"/opt/conda/lib/python3.9/site-packages/keras/engine/functional.py\", line 589, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"/opt/conda/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/opt/conda/lib/python3.9/site-packages/keras/engine/base_layer.py\", line 1096, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/opt/conda/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"/tmp/ipykernel_2023/3326115625.py\", line 30, in call\n      attention_out = self.attention(inputs, inputs, attention_mask=causal_mask)\n    File \"/opt/conda/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/opt/conda/lib/python3.9/site-packages/keras/engine/base_layer.py\", line 1096, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/opt/conda/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"/opt/conda/lib/python3.9/site-packages/keras/layers/multi_head_attention.py\", line 505, in call\n      key = self._key_dense(key)\n    File \"/opt/conda/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/opt/conda/lib/python3.9/site-packages/keras/engine/base_layer.py\", line 1096, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/opt/conda/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"/opt/conda/lib/python3.9/site-packages/keras/layers/einsum_dense.py\", line 187, in call\n      ret = tf.einsum(self.equation, inputs, self.kernel)\nNode: 'model/transformer/multi_head_attention/key/einsum/Einsum'\nAttempting to perform BLAS operation using StreamExecutor without BLAS support\n\t [[{{node model/transformer/multi_head_attention/key/einsum/Einsum}}]] [Op:__inference_train_function_2840]"
     ]
    }
   ],
   "source": [
    "# wandb.tensorboard.patch(root_logdir=\"logs\")\n",
    "# wandb.init(project='transformer')\n",
    "model.fit(dataset, verbose=2, epochs=T_EPOCHS, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model.save(\"model_warmup.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.functional.Functional at 0x7f9798641100>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from official import nlp\n",
    "from official.nlp.modeling.ops import beam_search, sampling_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {}\n",
    "params[\"num_heads\"] = M_ATT_HEADS\n",
    "params[\"num_layers\"] = 1\n",
    "params[\"batch_size\"] = T_BATCH_SIZE\n",
    "params[\"n_dims\"] = M_DIM_EMB\n",
    "params[\"max_decode_length\"] = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cache key shape for layer 0 : (256, 100, 2, 128)\n"
     ]
    }
   ],
   "source": [
    "cache = {\n",
    "    \"layer_%d\"\n",
    "    % layer: {\n",
    "        \"k\": tf.zeros(\n",
    "            [\n",
    "                params[\"batch_size\"],\n",
    "                params[\"max_decode_length\"],\n",
    "                params[\"num_heads\"],\n",
    "                int(params[\"n_dims\"] / params[\"num_heads\"]),\n",
    "            ],\n",
    "            dtype=tf.float32,\n",
    "        ),\n",
    "        \"v\": tf.zeros(\n",
    "            [\n",
    "                params[\"batch_size\"],\n",
    "                params[\"max_decode_length\"],\n",
    "                params[\"num_heads\"],\n",
    "                int(params[\"n_dims\"] / params[\"num_heads\"]),\n",
    "            ],\n",
    "            dtype=tf.float32,\n",
    "        ),\n",
    "    }\n",
    "    for layer in range(params[\"num_layers\"])\n",
    "}\n",
    "print(\"cache key shape for layer 0 :\", cache[\"layer_0\"][\"k\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def length_norm(length, dtype):\n",
    "    \"\"\"Return length normalization factor.\"\"\"\n",
    "    return tf.pow(((5.0 + tf.cast(length, dtype)) / 6.0), 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn(ids):\n",
    "    y, _ = model.predict(ids)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _symbols_to_logits_fn():\n",
    "    \"\"\"Calculates logits of the next tokens.\"\"\"\n",
    "\n",
    "    def symbols_to_logits_fn(ids, i, temp_cache):\n",
    "        # Get the tensor into a numpy array\n",
    "        ids = tf.reshape(ids, [1, -1]).numpy()\n",
    "        # Pad the array to the max sequence lenght\n",
    "        l = len(ids[0])\n",
    "        ids = np.pad(\n",
    "            np.array(ids),\n",
    "            [(0, 0), (0, M_MAX_LEN - l)],\n",
    "            mode=\"constant\",\n",
    "            constant_values=0,\n",
    "        )\n",
    "        print(ids)\n",
    "        # Feed through the model\n",
    "        # logits = tf.cast(tf.math.log(model_fn(ids)), tf.float32)\n",
    "        logits = model_fn(ids)\n",
    "\n",
    "        # Return the generated logits\n",
    "        print(\"-\" * 80)\n",
    "        print(logits[0])\n",
    "        return logits, temp_cache\n",
    "\n",
    "    return symbols_to_logits_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_prompt(prompt, vocab, max_len):\n",
    "    word_to_index = {word: index for index, word in enumerate(vocab)}\n",
    "    prompt_tokens = [word_to_index.get(_, 1) for _ in prompt.lower().split()]\n",
    "    return prompt_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detokenize(vocab, number):\n",
    "    return vocab[number]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 150, 187]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_enc = tokenize_prompt(\"i will always\", vocab, M_MAX_LEN)\n",
    "prompt_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  3 150 187   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-17 00:25:30.502476: E tensorflow/stream_executor/cuda/cuda_blas.cc:232] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\n",
      "2022-05-17 00:25:30.502513: E tensorflow/stream_executor/cuda/cuda_blas.cc:234] Failure to initialize cublas may be due to OOM (cublas needs some free memory when you initialize it, and your deep-learning framework may have preallocated more than its fair share), or may be because this binary was not built with support for the GPU in your machine.\n",
      "2022-05-17 00:25:30.502538: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at matmul_op_impl.h:438 : INTERNAL: Attempting to perform BLAS operation using StreamExecutor without BLAS support\n",
      "2022-05-17 00:25:30.539862: E tensorflow/stream_executor/cuda/cuda_blas.cc:232] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\n",
      "2022-05-17 00:25:30.539925: E tensorflow/stream_executor/cuda/cuda_blas.cc:234] Failure to initialize cublas may be due to OOM (cublas needs some free memory when you initialize it, and your deep-learning framework may have preallocated more than its fair share), or may be because this binary was not built with support for the GPU in your machine.\n",
      "2022-05-17 00:25:30.539966: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at matmul_op_impl.h:438 : INTERNAL: Attempting to perform BLAS operation using StreamExecutor without BLAS support\n",
      "2022-05-17 00:25:30.556833: E tensorflow/stream_executor/cuda/cuda_blas.cc:232] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\n",
      "2022-05-17 00:25:30.556992: E tensorflow/stream_executor/cuda/cuda_blas.cc:234] Failure to initialize cublas may be due to OOM (cublas needs some free memory when you initialize it, and your deep-learning framework may have preallocated more than its fair share), or may be because this binary was not built with support for the GPU in your machine.\n",
      "2022-05-17 00:25:30.557058: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at matmul_op_impl.h:438 : INTERNAL: Attempting to perform BLAS operation using StreamExecutor without BLAS support\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Graph execution error:\n\nDetected at node 'model_1/transformer_1/multi_head_attention_1/key/einsum/Einsum' defined at (most recent call last):\n    File \"/opt/conda/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/opt/conda/lib/python3.9/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/opt/conda/lib/python3.9/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/opt/conda/lib/python3.9/site-packages/traitlets/config/application.py\", line 846, in launch_instance\n      app.start()\n    File \"/opt/conda/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"/opt/conda/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"/opt/conda/lib/python3.9/asyncio/base_events.py\", line 596, in run_forever\n      self._run_once()\n    File \"/opt/conda/lib/python3.9/asyncio/base_events.py\", line 1890, in _run_once\n      handle._run()\n    File \"/opt/conda/lib/python3.9/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/opt/conda/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 504, in dispatch_queue\n      await self.process_one()\n    File \"/opt/conda/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 493, in process_one\n      await dispatch(*args)\n    File \"/opt/conda/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 400, in dispatch_shell\n      await result\n    File \"/opt/conda/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 724, in execute_request\n      reply_content = await reply_content\n    File \"/opt/conda/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 390, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"/opt/conda/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/opt/conda/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2863, in run_cell\n      result = self._run_cell(\n    File \"/opt/conda/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2909, in _run_cell\n      return runner(coro)\n    File \"/opt/conda/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/opt/conda/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3106, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/opt/conda/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3309, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/opt/conda/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3369, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_12206/3575384082.py\", line 13, in <cell line: 13>\n      ids, _ = top_k_obj.generate(initial_ids=prompt_enc, initial_cache={})\n    File \"/opt/conda/lib/python3.9/site-packages/official/nlp/modeling/ops/decoding_module.py\", line 167, in generate\n      tf.while_loop(\n    File \"/opt/conda/lib/python3.9/site-packages/official/nlp/modeling/ops/decoding_module.py\", line 146, in _generate_step\n      topk_seq, topk_log_probs, topk_ids, new_cache = self._grow_alive_seq(\n    File \"/opt/conda/lib/python3.9/site-packages/official/nlp/modeling/ops/sampling_module.py\", line 209, in _grow_alive_seq\n      new_logits, new_cache = self.symbols_to_logits_fn(ids, i, alive_cache)\n    File \"/tmp/ipykernel_12206/376583282.py\", line 18, in symbols_to_logits_fn\n      logits = model_fn(ids)\n    File \"/tmp/ipykernel_12206/2936490992.py\", line 2, in model_fn\n      y, _ = model.predict(ids)\n    File \"/opt/conda/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/opt/conda/lib/python3.9/site-packages/keras/engine/training.py\", line 2033, in predict\n      tmp_batch_outputs = self.predict_function(iterator)\n    File \"/opt/conda/lib/python3.9/site-packages/keras/engine/training.py\", line 1845, in predict_function\n      return step_function(self, iterator)\n    File \"/opt/conda/lib/python3.9/site-packages/keras/engine/training.py\", line 1834, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/conda/lib/python3.9/site-packages/keras/engine/training.py\", line 1823, in run_step\n      outputs = model.predict_step(data)\n    File \"/opt/conda/lib/python3.9/site-packages/keras/engine/training.py\", line 1791, in predict_step\n      return self(x, training=False)\n    File \"/opt/conda/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/opt/conda/lib/python3.9/site-packages/keras/engine/training.py\", line 490, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/opt/conda/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/opt/conda/lib/python3.9/site-packages/keras/engine/base_layer.py\", line 1014, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/opt/conda/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"/opt/conda/lib/python3.9/site-packages/keras/engine/functional.py\", line 458, in call\n      return self._run_internal_graph(\n    File \"/opt/conda/lib/python3.9/site-packages/keras/engine/functional.py\", line 596, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"/opt/conda/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/opt/conda/lib/python3.9/site-packages/keras/engine/base_layer.py\", line 1014, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/opt/conda/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"/tmp/ipykernel_12206/2577669967.py\", line 35, in call\n      attention_out = self.attention(inputs, inputs, attention_mask=causal_mask)\n    File \"/opt/conda/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/opt/conda/lib/python3.9/site-packages/keras/engine/base_layer.py\", line 1014, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/opt/conda/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"/opt/conda/lib/python3.9/site-packages/keras/layers/attention/multi_head_attention.py\", line 504, in call\n      key = self._key_dense(key)\n    File \"/opt/conda/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/opt/conda/lib/python3.9/site-packages/keras/engine/base_layer.py\", line 1014, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/opt/conda/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"/opt/conda/lib/python3.9/site-packages/keras/layers/einsum_dense.py\", line 187, in call\n      ret = tf.einsum(self.equation, inputs, self.kernel)\nNode: 'model_1/transformer_1/multi_head_attention_1/key/einsum/Einsum'\nAttempting to perform BLAS operation using StreamExecutor without BLAS support\n\t [[{{node model_1/transformer_1/multi_head_attention_1/key/einsum/Einsum}}]] [Op:__inference_predict_function_2991]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[0;32mIn [39]\u001b[0m, in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m top_k_obj \u001b[38;5;241m=\u001b[39m sampling_module\u001b[38;5;241m.\u001b[39mSamplingModule(\n\u001b[1;32m      2\u001b[0m     length_normalization_fn\u001b[38;5;241m=\u001b[39mlength_norm,\n\u001b[1;32m      3\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mfloat32,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m     enable_greedy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     12\u001b[0m )\n\u001b[0;32m---> 13\u001b[0m ids, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtop_k_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minitial_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt_enc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/official/nlp/modeling/ops/decoding_module.py:167\u001b[0m, in \u001b[0;36mDecodingModule.generate\u001b[0;34m(self, initial_ids, initial_cache)\u001b[0m\n\u001b[1;32m    162\u001b[0m   new_state\u001b[38;5;241m.\u001b[39mupdate(finished_state)\n\u001b[1;32m    163\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m [new_state]\n\u001b[1;32m    165\u001b[0m finished_state \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mmap_structure(\n\u001b[1;32m    166\u001b[0m     tf\u001b[38;5;241m.\u001b[39mstop_gradient,\n\u001b[0;32m--> 167\u001b[0m     \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhile_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_continue_search\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_generate_step\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloop_vars\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshape_invariants\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mstate_shapes\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparallel_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    173\u001b[0m final_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_finished_state(finished_state[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m final_state\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/util/deprecation.py:629\u001b[0m, in \u001b[0;36mdeprecated_arg_values.<locals>.deprecated_wrapper.<locals>.new_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    622\u001b[0m           _PRINTED_WARNING[(func, arg_name)] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    623\u001b[0m         logging\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    624\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFrom \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: calling \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m (from \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m) with \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is deprecated and \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    625\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwill be removed \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mInstructions for updating:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    626\u001b[0m             _call_location(), decorator_utils\u001b[38;5;241m.\u001b[39mget_qualified_name(func),\n\u001b[1;32m    627\u001b[0m             func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m, arg_name, arg_value, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124min a future version\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    628\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m date \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m date), instructions)\n\u001b[0;32m--> 629\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/ops/control_flow_ops.py:2507\u001b[0m, in \u001b[0;36mwhile_loop_v2\u001b[0;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, maximum_iterations, name)\u001b[0m\n\u001b[1;32m   2331\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhile_loop\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[])\n\u001b[1;32m   2332\u001b[0m \u001b[38;5;129m@deprecation\u001b[39m\u001b[38;5;241m.\u001b[39mdeprecated_arg_values(\n\u001b[1;32m   2333\u001b[0m     \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2348\u001b[0m                   maximum_iterations\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   2349\u001b[0m                   name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   2350\u001b[0m   \u001b[38;5;124;03m\"\"\"Repeat `body` while the condition `cond` is true.\u001b[39;00m\n\u001b[1;32m   2351\u001b[0m \n\u001b[1;32m   2352\u001b[0m \u001b[38;5;124;03m  `cond` is a callable returning a boolean scalar tensor. `body` is a callable\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2505\u001b[0m \n\u001b[1;32m   2506\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2507\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwhile_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2508\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcond\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcond\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2509\u001b[0m \u001b[43m      \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2510\u001b[0m \u001b[43m      \u001b[49m\u001b[43mloop_vars\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloop_vars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2511\u001b[0m \u001b[43m      \u001b[49m\u001b[43mshape_invariants\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshape_invariants\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2512\u001b[0m \u001b[43m      \u001b[49m\u001b[43mparallel_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparallel_iterations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2513\u001b[0m \u001b[43m      \u001b[49m\u001b[43mback_prop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mback_prop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2514\u001b[0m \u001b[43m      \u001b[49m\u001b[43mswap_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mswap_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2515\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2516\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmaximum_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximum_iterations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2517\u001b[0m \u001b[43m      \u001b[49m\u001b[43mreturn_same_structure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/ops/control_flow_ops.py:2754\u001b[0m, in \u001b[0;36mwhile_loop\u001b[0;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations, return_same_structure)\u001b[0m\n\u001b[1;32m   2751\u001b[0m loop_var_structure \u001b[38;5;241m=\u001b[39m nest\u001b[38;5;241m.\u001b[39mmap_structure(type_spec\u001b[38;5;241m.\u001b[39mtype_spec_from_value,\n\u001b[1;32m   2752\u001b[0m                                         \u001b[38;5;28mlist\u001b[39m(loop_vars))\n\u001b[1;32m   2753\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m cond(\u001b[38;5;241m*\u001b[39mloop_vars):\n\u001b[0;32m-> 2754\u001b[0m   loop_vars \u001b[38;5;241m=\u001b[39m \u001b[43mbody\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mloop_vars\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2755\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m try_to_pack \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(loop_vars, (\u001b[38;5;28mlist\u001b[39m, _basetuple)):\n\u001b[1;32m   2756\u001b[0m     packed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/official/nlp/modeling/ops/decoding_module.py:146\u001b[0m, in \u001b[0;36mDecodingModule.generate.<locals>._generate_step\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_step\u001b[39m(state):\n\u001b[0;32m--> 146\u001b[0m   topk_seq, topk_log_probs, topk_ids, new_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_grow_alive_seq\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m      \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m   new_finished_flags \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_finished_flags(topk_ids, state)\n\u001b[1;32m    149\u001b[0m   alive_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_new_alive_state(topk_seq,\n\u001b[1;32m    150\u001b[0m                                           topk_log_probs,\n\u001b[1;32m    151\u001b[0m                                           new_finished_flags,\n\u001b[1;32m    152\u001b[0m                                           new_cache)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/official/nlp/modeling/ops/sampling_module.py:209\u001b[0m, in \u001b[0;36mSamplingModule._grow_alive_seq\u001b[0;34m(self, state, batch_size)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m   ids \u001b[38;5;241m=\u001b[39m alive_seq\n\u001b[0;32m--> 209\u001b[0m new_logits, new_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msymbols_to_logits_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malive_cache\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    210\u001b[0m candidate_log_probs \u001b[38;5;241m=\u001b[39m decoding_module\u001b[38;5;241m.\u001b[39mlog_prob_from_logits(\n\u001b[1;32m    211\u001b[0m     new_logits)\n\u001b[1;32m    212\u001b[0m original_log_probs \u001b[38;5;241m=\u001b[39m candidate_log_probs \u001b[38;5;241m+\u001b[39m alive_log_probs\n",
      "Input \u001b[0;32mIn [35]\u001b[0m, in \u001b[0;36m_symbols_to_logits_fn.<locals>.symbols_to_logits_fn\u001b[0;34m(ids, i, temp_cache)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(ids)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Feed through the model\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# logits = tf.cast(tf.math.log(model_fn(ids)), tf.float32)\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Return the generated logits\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m80\u001b[39m)\n",
      "Input \u001b[0;32mIn [24]\u001b[0m, in \u001b[0;36mmodel_fn\u001b[0;34m(ids)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmodel_fn\u001b[39m(ids):\n\u001b[0;32m----> 2\u001b[0m     y, _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mInternalError\u001b[0m: Graph execution error:\n\nDetected at node 'model_1/transformer_1/multi_head_attention_1/key/einsum/Einsum' defined at (most recent call last):\n    File \"/opt/conda/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/opt/conda/lib/python3.9/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/opt/conda/lib/python3.9/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/opt/conda/lib/python3.9/site-packages/traitlets/config/application.py\", line 846, in launch_instance\n      app.start()\n    File \"/opt/conda/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"/opt/conda/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"/opt/conda/lib/python3.9/asyncio/base_events.py\", line 596, in run_forever\n      self._run_once()\n    File \"/opt/conda/lib/python3.9/asyncio/base_events.py\", line 1890, in _run_once\n      handle._run()\n    File \"/opt/conda/lib/python3.9/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/opt/conda/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 504, in dispatch_queue\n      await self.process_one()\n    File \"/opt/conda/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 493, in process_one\n      await dispatch(*args)\n    File \"/opt/conda/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 400, in dispatch_shell\n      await result\n    File \"/opt/conda/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 724, in execute_request\n      reply_content = await reply_content\n    File \"/opt/conda/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 390, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"/opt/conda/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/opt/conda/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2863, in run_cell\n      result = self._run_cell(\n    File \"/opt/conda/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2909, in _run_cell\n      return runner(coro)\n    File \"/opt/conda/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/opt/conda/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3106, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/opt/conda/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3309, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/opt/conda/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3369, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_12206/3575384082.py\", line 13, in <cell line: 13>\n      ids, _ = top_k_obj.generate(initial_ids=prompt_enc, initial_cache={})\n    File \"/opt/conda/lib/python3.9/site-packages/official/nlp/modeling/ops/decoding_module.py\", line 167, in generate\n      tf.while_loop(\n    File \"/opt/conda/lib/python3.9/site-packages/official/nlp/modeling/ops/decoding_module.py\", line 146, in _generate_step\n      topk_seq, topk_log_probs, topk_ids, new_cache = self._grow_alive_seq(\n    File \"/opt/conda/lib/python3.9/site-packages/official/nlp/modeling/ops/sampling_module.py\", line 209, in _grow_alive_seq\n      new_logits, new_cache = self.symbols_to_logits_fn(ids, i, alive_cache)\n    File \"/tmp/ipykernel_12206/376583282.py\", line 18, in symbols_to_logits_fn\n      logits = model_fn(ids)\n    File \"/tmp/ipykernel_12206/2936490992.py\", line 2, in model_fn\n      y, _ = model.predict(ids)\n    File \"/opt/conda/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/opt/conda/lib/python3.9/site-packages/keras/engine/training.py\", line 2033, in predict\n      tmp_batch_outputs = self.predict_function(iterator)\n    File \"/opt/conda/lib/python3.9/site-packages/keras/engine/training.py\", line 1845, in predict_function\n      return step_function(self, iterator)\n    File \"/opt/conda/lib/python3.9/site-packages/keras/engine/training.py\", line 1834, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/conda/lib/python3.9/site-packages/keras/engine/training.py\", line 1823, in run_step\n      outputs = model.predict_step(data)\n    File \"/opt/conda/lib/python3.9/site-packages/keras/engine/training.py\", line 1791, in predict_step\n      return self(x, training=False)\n    File \"/opt/conda/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/opt/conda/lib/python3.9/site-packages/keras/engine/training.py\", line 490, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/opt/conda/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/opt/conda/lib/python3.9/site-packages/keras/engine/base_layer.py\", line 1014, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/opt/conda/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"/opt/conda/lib/python3.9/site-packages/keras/engine/functional.py\", line 458, in call\n      return self._run_internal_graph(\n    File \"/opt/conda/lib/python3.9/site-packages/keras/engine/functional.py\", line 596, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"/opt/conda/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/opt/conda/lib/python3.9/site-packages/keras/engine/base_layer.py\", line 1014, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/opt/conda/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"/tmp/ipykernel_12206/2577669967.py\", line 35, in call\n      attention_out = self.attention(inputs, inputs, attention_mask=causal_mask)\n    File \"/opt/conda/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/opt/conda/lib/python3.9/site-packages/keras/engine/base_layer.py\", line 1014, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/opt/conda/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"/opt/conda/lib/python3.9/site-packages/keras/layers/attention/multi_head_attention.py\", line 504, in call\n      key = self._key_dense(key)\n    File \"/opt/conda/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/opt/conda/lib/python3.9/site-packages/keras/engine/base_layer.py\", line 1014, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/opt/conda/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"/opt/conda/lib/python3.9/site-packages/keras/layers/einsum_dense.py\", line 187, in call\n      ret = tf.einsum(self.equation, inputs, self.kernel)\nNode: 'model_1/transformer_1/multi_head_attention_1/key/einsum/Einsum'\nAttempting to perform BLAS operation using StreamExecutor without BLAS support\n\t [[{{node model_1/transformer_1/multi_head_attention_1/key/einsum/Einsum}}]] [Op:__inference_predict_function_2991]"
     ]
    }
   ],
   "source": [
    "top_k_obj = sampling_module.SamplingModule(\n",
    "    length_normalization_fn=length_norm,\n",
    "    dtype=tf.float32,\n",
    "    symbols_to_logits_fn=_symbols_to_logits_fn(),\n",
    "    vocab_size=M_VOCAB_SZ,\n",
    "    max_decode_length=params[\"max_decode_length\"],\n",
    "    eos_id=0,\n",
    "    sample_temperature=tf.constant(1.0),\n",
    "    top_k=tf.constant(3),\n",
    "    padded_decode=False,\n",
    "    enable_greedy=False,\n",
    ")\n",
    "ids, _ = top_k_obj.generate(initial_ids=prompt_enc, initial_cache={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  1, 23], dtype=int32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant([1, 1, 23]).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = len([1, 1, 23])\n",
    "np.pad(\n",
    "    np.array([1, 1, 23]), (0, M_VOCAB_SZ - l), mode=\"constant\", constant_values=0\n",
    ").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_VOCAB_SZ = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMAhLApJ+cG6CCp2Dad8FXD",
   "collapsed_sections": [],
   "name": "transformer.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
